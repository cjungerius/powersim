[
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Power Simulation in a Mixed Effects design using Python",
    "section": "",
    "text": "Before we do anything, let’s import all the packages we will need:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport seaborn.objects as so\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport itertools\nimport glob, os, warnings\nfrom sklearn.model_selection import ParameterGrid\n\nIn this example, we will make an estimate of the number of participants we need to replicate a simple and well-established experimental finding: The capture of attention by a colour singleton during visual search for a unique shape singleton. For this example, we are fortunate in that there are many studies of this effect for us to base our parameter estimates on. One recent example is a highly-powered study from the Serences lab purpose-built to be used for sensitivity analysis. First let’s import the data for our specific case from the Adam et al. (2021) study, which is freely available in an OSF repository, and look at the data.\nNote that when previous data doesn’t exist (or even if it does, but you don’t trust that it’s sufficient to base your effect estimates on) there are alternative ways of determining such parameters, including formally determining a smallest effect size of interest.\nThe data we chose is from experiment 1c: variable colour singleton search. We are interested in the raw trial data, not the summary data (We are doing a mixed model after all, not an ANOVA) so we have to grab all the raw files and concatenate them.\n\npath = os.path.join(os.getcwd(),\"Experiment_1c\")\nall_files = glob.glob(os.path.join(path, \"*.csv\"))\ndf = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n\nOnce it’s imported, we can take a look at our data, e.g., looking at subject means between the two conditions:\n\nd = (df\n  .query(\"acc==1 & set_size==4\")\n  .assign(rt = lambda x: x.rt * 1000))\n\nsns.set_theme()\n\nsns.catplot(\n  data=d,\n  x=\"distractor\",\n  y=\"rt\",\n  hue=\"subject\",\n  kind=\"point\"\n)\n\n<seaborn.axisgrid.FacetGrid at 0x1e48f960988>\n\n\n\n\n\nWe can clearly see typical atttentional capture effects in the data. Now that we have the data, let’s model it:\n\nd = (df\n  .query(\"acc==1 & set_size==4\")\n  .assign(rt = lambda x: x.rt * 1000))\n\n# Our model is simple: RT is dependent on distractor presence, with a random slope and intercept for each subject. More complex models are left as an exercise to the reader.\n\nmd = smf.mixedlm(\"rt ~ distractor\", d, groups=d['subject'],re_formula=\"~distractor\")\nmdf = md.fit()\nmdf.summary()\n\n\n\n\n       Model:       MixedLM Dependent Variable:     rt     \n\n\n  No. Observations:  9164         Method:          REML    \n\n\n     No. Groups:      24          Scale:        30910.5844 \n\n\n  Min. group size:    353     Log-Likelihood:   -60427.4618\n\n\n  Max. group size:    395       Converged:          Yes    \n\n\n  Mean group size:   381.8                                 \n\n\n\n\n                                      Coef.  Std.Err.    z   P>|z| [0.025  0.975] \n\n\n  Intercept                          651.500  16.663  39.098 0.000 618.841 684.159\n\n\n  distractor[T.present]              30.857    4.753   6.493 0.000 21.542  40.171 \n\n\n  Group Var                         6501.876  11.192                              \n\n\n  Group x distractor[T.present] Cov  416.478   2.281                              \n\n\n  distractor[T.present] Var          218.053   0.912                              \n\n\n\n\nThe above model rt ~ distractor + ( distractor | subject) is our putative data generating process, the parameters that we believe underly the generation of observed dependent variables, and the relationship between those parameters. The table shown above gives us parameter estimates for all fixed and random effects in the model. Now let’s plug those parameters into a simulation!\n\nn_subj     = 10   # number of subjects\nn_present  = 200   # number of distractor present trials\nn_absent   = 200   # number of distractor absent\nbeta_0     = 650   # grand mean\nbeta_1     = 30   # effect of category\ntau_0      = 80   # by-subject random intercept sd\ntau_1      = 15   # by-subject random slope sd\nrho        = 0.35   # correlation between intercept and slope\nsigma      = 175 # residual nose\n\nGenerate trials with their fixed effects:\n\n# simulate a sample of items\n# total number of items = n_ingroup + n_outgroup\n\nitems = (pd\n  .DataFrame({\n    'item_id': range(n_absent+n_present),\n    'category' : np.repeat(['absent', 'present'], [n_absent, n_present])})\n  .assign(X_i=lambda x: np.where(x[\"category\"] == 'present', 0.5, -0.5)))\n\nitems.describe()\n\n\n\n\n\n  \n    \n      \n      item_id\n      X_i\n    \n  \n  \n    \n      count\n      400.000000\n      400.000000\n    \n    \n      mean\n      199.500000\n      0.000000\n    \n    \n      std\n      115.614301\n      0.500626\n    \n    \n      min\n      0.000000\n      -0.500000\n    \n    \n      25%\n      99.750000\n      -0.500000\n    \n    \n      50%\n      199.500000\n      0.000000\n    \n    \n      75%\n      299.250000\n      0.500000\n    \n    \n      max\n      399.000000\n      0.500000\n    \n  \n\n\n\n\nAnd generate participants with their random intercepts and slopes:\n\n# simulate a sample of subjects\n\n# calculate random intercept / random slope covariance\ncovar = rho * tau_0 * tau_1\n\n# put values into variance-covariance matrix\ncov_mx = np.array([\n  [tau_0**2, covar], \n  [covar, tau_1**2]\n])\n\n# generate the by-subject random effects\nsubject_rfx = np.random.multivariate_normal(mean = [0, 0], cov = cov_mx, size = n_subj)\n\n# combine with subject IDs\nsubjects = pd.DataFrame({\n  'subj_id': range(n_subj),\n  'T_0s': subject_rfx[:,0],\n  'T_1s': subject_rfx[:,1]\n})\n\nsubjects.describe()\n\n\n\n\n\n  \n    \n      \n      subj_id\n      T_0s\n      T_1s\n    \n  \n  \n    \n      count\n      10.00000\n      10.000000\n      10.000000\n    \n    \n      mean\n      4.50000\n      4.504165\n      -1.873139\n    \n    \n      std\n      3.02765\n      73.902334\n      16.003082\n    \n    \n      min\n      0.00000\n      -142.374490\n      -24.238591\n    \n    \n      25%\n      2.25000\n      -24.674344\n      -18.007176\n    \n    \n      50%\n      4.50000\n      28.476472\n      3.896861\n    \n    \n      75%\n      6.75000\n      60.661848\n      9.098724\n    \n    \n      max\n      9.00000\n      73.153287\n      20.876987\n    \n  \n\n\n\n\nNow combine and add residual noise to create a complete dataframe:\n\n#cross items and subjects, add noise\nitems['key'] = 1\nsubjects['key'] = 1\ntrials = (pd\n  .merge(items,subjects, on='key')\n  .drop(\"key\",axis=1)\n  .assign(e_si = lambda x: np.random.normal(scale=sigma,size=len(x))))\n\n\n# calculate the response variable\ndat_sim = (trials\n  .assign(RT = lambda x: beta_0 + x.T_0s + (beta_1 + x.T_1s) * x.X_i + x.e_si)\n  .filter(items=['subj_id', 'item_id', 'category', 'X_i', 'RT']))\n\ndat_sim.head(10)\n\n\n\n\n\n  \n    \n      \n      subj_id\n      item_id\n      category\n      X_i\n      RT\n    \n  \n  \n    \n      0\n      0\n      0\n      absent\n      -0.5\n      564.972949\n    \n    \n      1\n      1\n      0\n      absent\n      -0.5\n      609.276783\n    \n    \n      2\n      2\n      0\n      absent\n      -0.5\n      474.376545\n    \n    \n      3\n      3\n      0\n      absent\n      -0.5\n      798.241290\n    \n    \n      4\n      4\n      0\n      absent\n      -0.5\n      780.960725\n    \n    \n      5\n      5\n      0\n      absent\n      -0.5\n      418.337093\n    \n    \n      6\n      6\n      0\n      absent\n      -0.5\n      453.806666\n    \n    \n      7\n      7\n      0\n      absent\n      -0.5\n      403.769000\n    \n    \n      8\n      8\n      0\n      absent\n      -0.5\n      657.886429\n    \n    \n      9\n      9\n      0\n      absent\n      -0.5\n      165.095569\n    \n  \n\n\n\n\nData generated! Does it look like we’d expect?\n\nsns.catplot(\n  data=dat_sim,\n  x=\"category\",\n  y=\"RT\",\n  hue=\"subj_id\",\n  kind=\"point\"\n)\n\n<seaborn.axisgrid.FacetGrid at 0x1e48922f688>\n\n\n\n\n\nLooks comparable to the original data! Now let’s fit a model to see if we recover the parameters:\n\nmd = smf.mixedlm(\"RT ~ category\", dat_sim, groups=dat_sim['subj_id'],  re_formula=\"~category\")\nmdf = md.fit()\nmdf.summary()\n\n\n\n\n       Model:       MixedLM Dependent Variable:     RT     \n\n\n  No. Observations:  4000         Method:          REML    \n\n\n     No. Groups:      10          Scale:        31304.7050 \n\n\n  Min. group size:    400     Log-Likelihood:   -26393.4270\n\n\n  Max. group size:    400       Converged:          Yes    \n\n\n  Mean group size:   400.0                                 \n\n\n\n\n                                    Coef.  Std.Err.    z   P>|z| [0.025  0.975] \n\n\n  Intercept                        639.313  23.865  26.788 0.000 592.538 686.089\n\n\n  category[T.present]              25.278    6.811   3.711 0.000 11.928  38.628 \n\n\n  Group Var                       5539.037  15.196                              \n\n\n  Group x category[T.present] Cov -517.669   3.318                              \n\n\n  category[T.present] Var          150.885   1.238                              \n\n\n\n\nGreat, our simulation works - our fixed effect parameter estimates are close to the originals, and statistically significant! Now for a power analysis, we’d put the above in functions and run the code many times for a given combination of parameters. See below:\n\ndef my_sim_data(\n  n_subj     = 5,   # number of subjects\n  n_present  = 200,   # number of distractor present trials\n  n_absent   = 200,   # number of distractor absent\n  beta_0     = 650,   # grand mean\n  beta_1     = 30,   # effect of category\n  tau_0      = 80,   # by-subject random intercept sd\n  tau_1      = 15,   # by-subject random slope sd\n  rho        = 0.35,   # correlation between intercept and slope\n  sigma      = 175\n  ):\n  \n\n  # simulate a sample of items\n  # total number of items = n_ingroup + n_outgroup\n  items = (pd.DataFrame({\n  'item_id': range(n_absent+n_present),\n  'category' : np.repeat(['absent', 'present'], [n_absent, n_present])\n})\n  .assign(X_i=lambda x: np.where(x[\"category\"] == 'present', 0.5, -0.5)))\n\n\n  # simulate a sample of subjects\n\n  # calculate random intercept / random slope covariance\n  covar = rho * tau_0 * tau_1\n\n  # put values into variance-covariance matrix\n  cov_mx = np.array([\n    [tau_0**2, covar], \n    [covar, tau_1**2]\n  ])\n\n  # generate the by-subject random effects\n  subject_rfx = np.random.multivariate_normal(mean = [0, 0], cov = cov_mx, size = n_subj)\n\n  # combine with subject IDs\n  subjects = pd.DataFrame({\n    'subj_id': range(n_subj),\n    'T_0s': subject_rfx[:,0],\n    'T_1s': subject_rfx[:,1]\n  })\n\n  #cross items and subjects, add noise\n  items['key'] = 1\n  subjects['key'] = 1\n  trials = (pd\n    .merge(items,subjects, on='key').drop(\"key\",axis=1)\n    .assign(e_si = lambda x: np.random.normal(scale=sigma,size=len(x))))\n\n\n  # calculate the response variable\n  dat_sim = (trials\n    .assign(RT = lambda x: beta_0 + x.T_0s + (beta_1 + x.T_1s) * x.X_i + x.e_si)\n    .filter(items=['subj_id', 'item_id', 'category', 'X_i', 'RT']))\n\n  return dat_sim\n\nThe above function simulates data. The function below combines it with a model fit so we have a function that can be repeatedly called during our power analysis.\n\ndef single_run(filename = None, *args, **kwargs):\n  dat_sim = my_sim_data(*args, **kwargs)\n  with warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"ignore\")\n    md = smf.mixedlm(\"RT ~ category\", dat_sim, groups=dat_sim['subj_id'], re_formula=\"~category\")\n    mod_sim = md.fit()\n  sim_results = (mod_sim\n    .summary()\n    .tables[1]\n    .assign(**kwargs))\n  sim_results = sim_results.apply(pd.to_numeric, errors='coerce')\n  sim_results.index.rename('i',inplace=True)\n\n  if not filename == None:\n    hdr = not os.path.isfile(filename)\n    sim_results.to_csv(filename, mode='a',header=hdr)\n  \n  return sim_results\n\nNow let’s run our sensitivity analysis - we will run our simulation many times (100 times here for speed, but aim for more, like 1000+) for each combination of parameters, and record how often the fixed effect estimates reach significance:\n\nnreps = 100\n\nparams = ParameterGrid({\n  'n_subj'     : [5], # number of subjects\n  'n_present'  : [150],   # number of distractor present trials\n  'n_absent'   : [150],   # number of distractor absent\n  'beta_0'     : [650],   # grand mean\n  'beta_1'     : [30],   # effect of category\n  'tau_0'      : [80],   # by-subject random intercept sd\n  'tau_1'      : [15],   # by-subject random slope sd\n  'rho'        : [0.35],   # correlation between intercept and slope\n  'sigma'      : [175]  # residual (standard deviation)\n})\n\nsims = pd.concat([single_run(**param) for param in params for i in range(nreps)])\n\n\n  \nalpha = 0.05\n\n(sims\n  .assign(power = sims['P>|z|'] < alpha)\n  .query('i==\"Intercept\" or i==\"category[T.present]\"')\n  .groupby(['i'])\n  .agg(\n      mean_estimate = ('Coef.','mean'),\n      mean_se = ('Coef.', 'sem'),\n      power = ('power', 'mean')))\n\n\n\n\n\n  \n    \n      \n      mean_estimate\n      mean_se\n      power\n    \n    \n      i\n      \n      \n      \n    \n  \n  \n    \n      Intercept\n      639.46148\n      4.077712\n      1.00\n    \n    \n      category[T.present]\n      29.27915\n      1.130166\n      0.64\n    \n  \n\n\n\n\nIf we want to run our sensitivity analysis across a given parameter space, we’ll have to map the function single_run to generate data across this space, for example, over a varying number of participants:\n\nfilename1 = \"sens_py.csv\"\nnreps = 100\n\nparams = ParameterGrid({\n  'n_subj'     : range(2,10), # number of subjects\n  'n_present'  : [150],   # number of distractor present trials\n  'n_absent'   : [150],   # number of distractor absent\n  'beta_0'     : [650],   # grand mean\n  'beta_1'     : [30],   # effect of category\n  'tau_0'      : [80],   # by-subject random intercept sd\n  'tau_1'      : [15],   # by-subject random slope sd\n  'rho'        : [0.35],   # correlation between intercept and slope\n  'sigma'      : [175]  # residual (standard deviation)\n})\n\n\n\nif not os.path.isfile(filename1):\n  # run a simulation for each row of params\n  # and save to a file on each rep\n  sims1 = pd.concat([single_run(**param, filename=filename1) for param in params for i in range(nreps)])\n\nNote that the above could obviously also be run over other dimensions of our parameter space, e.g. for different estimates of the fixed effects, amount of noise, number of trials, etc. etc., by changing the params list. How did we do? Let’s take a look at our power curve.\n\nsims1 = pd.read_csv('sens_py.csv')\n\npower1 = (sims1.assign(power = sims1['P>|z|'] < alpha)\n  .query('i==\"category[T.present]\"')\\\n  .groupby(['n_subj'])\\\n  .agg(\n    mean_estimate = ('Coef.','mean'),\n    mean_se = ('Coef.', 'sem'),\n    power = ('power', 'mean')))\n\n\nsns.regplot(x=power1.index, y=power1[\"power\"],lowess=True)\n\n<AxesSubplot:xlabel='n_subj', ylabel='power'>\n\n\n\n\n\nOur power analysis suggests that, with the parameters established above, we need ~8 or more participants to reliably detect an effect!\nThe code used above is specific to power analysis for mixed models, but the approach generalises to other methods too, of course! The above code can easily be wrangled to handle different model types (simply change the model definition in single_run and make sure to capture the right parameters), and even Bayesian approaches. (For a thorough example of doing power analysis with Bayesian methods and the awesome bayesian regression package brms, see this blog post).\nEven if the above code is spaghetti to you (Perhaps you prefer R?), I hope you will take away a few things from this tutorial:\n\nPower analysis is nothing more than testing whether we can recover the parameters of a hypothesised data-generating process reliably using our statistical test of choice.\nWe can determine the parameters for such a data-generating process in the same way we formulate hypotheses (and indeed, in some ways these two things are one and the same): we use our knowledge, intuition, and previous work to inform our decision-making.\nIf you have a hypothetical data-generating process, you can simulate data by simply formalising that process as code and letting it simulate a dataset\nSimulation can help you answer questions about your statistical approach that are difficult to answer with other tools"
  }
]