[
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Power Simulation in a Mixed Effects design using Python",
    "section": "",
    "text": "In this notebook we’ll go through a quick example of setting up a power analysis, using data from an existing, highly-powered study to make credible parameter estimates. The code for setting up a simulation is inspired by/shamelessly stolen from a great tutorial about this topic by DeBruine & Barr (2021) and Lisa DeBruine’s appendix on its application for sensitivity analysis DeBruine & Barr (2020). The aim of this tutorial is two-fold:\n\nTo demonstrate this approach for the most basic mixed model (using it only to deal with repeated measures - no nesting, no crossed random effects with stimuli types, etc.) which is a very common use of the technique for researchers in my immediate environment (visual attention research).\nTo translate this approach to different languages - although I love R and encourage everyone to use it for statistical analysis, Python remains in use by a sizeable number of researchers, and I would also like to introduce Julia as an alternative.\n\nBefore we do anything, let’s import all the packages we will need:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport itertools\nimport glob, os, warnings\nfrom sklearn.model_selection import ParameterGrid\nfrom scipy.stats import chi2\n\nimport random\nrandom.seed(90059)\n\nIn this example, we will make an estimate of the number of participants we need to replicate a simple and well-established experimental finding: The capture of attention by a colour singleton during visual search for a unique shape singleton. For this example, we are fortunate in that there are many studies of this effect for us to base our parameter estimates on. One recent example is a highly-powered study by Kirsten Adam from the Serences lab purpose-built to be used for sensitivity analysis. First let’s import the data for our specific case from the Adam et al. (2021) study, which is freely available in an OSF repository, and look at the data.\nNote that when previous data doesn’t exist (or even if it does, but you don’t trust that it’s sufficient to base your effect estimates on) there are alternative ways of determining such parameters, including formally determining a smallest effect size of interest Lakens et al. (2018).\nThe data we chose is from experiment 1c: variable colour singleton search. We are interested in the raw trial data, not the summary data (We are doing a mixed model after all, not an ANOVA) so we have to grab all the raw files and concatenate them.\n\npath = os.path.join(os.getcwd(),\"Experiment_1c\")\nall_files = glob.glob(os.path.join(path, \"*.csv\"))\ndf = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n\nOnce it’s imported, we can take a look at our data, e.g., looking at subject means between the two conditions:\n\nd = (df\n  .query(\"acc==1 & set_size==4\")\n  .assign(rt = lambda x: x.rt * 1000)\n  .astype({'subject': 'str'}))\n\nsns.set_theme()\n\nsns.catplot(\n  data=d,\n  x=\"distractor\",\n  y=\"rt\",\n  hue=\"subject\",\n  kind=\"point\",\n  legend=False\n)\n\n\n\n\nWe can clearly see typical atttentional capture effects in the data. Now that we have the data, let’s model it:\n\n# Our model is simple: RT is dependent on distractor presence, with a random slope and intercept for each subject. More complex models are left as an exercise to the reader.\n\nmd = smf.mixedlm(\"rt ~ distractor\", d, groups=d['subject'],re_formula=\"~distractor\")\nmd_null = smf.mixedlm(\"rt ~ 1\", d, groups=d['subject'],re_formula=\"~distractor\")\n# We fit lmms with ML rather than REML to obtain the p-value of the likelihood ratio test for inclusion of a fixed effect (which is preferable over the default Wald test used by mixedlm to estimate significance)\nm_lrt = md.fit(reml=False)\nm_null_lrt = md_null.fit(reml=False)\nlrt = 1 - chi2.cdf(-2*(m_null_lrt.llf - m_lrt.llf), 1)\n\nprint(f'A likelihood ratio test estimates the p-value for the inclusion of a coefficient for the presence of a singleton distractor to be {lrt}')\n\n# Whereas for better estimates of the random effects' variance, we fit lmms with reml\nm1 = md.fit()\nm1.summary()\n\nA likelihood ratio test estimates the p-value for the inclusion of a coefficient for the presence of a singleton distractor to be 5.657725046237516e-07\n\n\n\n\n\nModel:\nMixedLM\nDependent Variable:\nrt\n\n\nNo. Observations:\n9164\nMethod:\nREML\n\n\nNo. Groups:\n24\nScale:\n30910.5844\n\n\nMin. group size:\n353\nLog-Likelihood:\n-60427.4618\n\n\nMax. group size:\n395\nConverged:\nYes\n\n\nMean group size:\n381.8\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n651.500\n16.663\n39.098\n0.000\n618.841\n684.159\n\n\ndistractor[T.present]\n30.857\n4.753\n6.493\n0.000\n21.542\n40.171\n\n\nGroup Var\n6501.876\n11.192\n\n\n\n\n\n\nGroup x distractor[T.present] Cov\n416.478\n2.281\n\n\n\n\n\n\ndistractor[T.present] Var\n218.053\n0.912\n\n\n\n\n\n\n\n\n\nThe above model rt ~ distractor + ( distractor | subject) is our putative data generating process, the parameters that we believe underly the generation of observed dependent variables, and the relationship between those parameters. The table shown above gives us parameter estimates for all fixed and random effects in the model. Now let’s plug those parameters into a simulation!\n\nn_subj     = 10     # number of subjects\nn_present  = 200    # number of distractor present trials\nn_absent   = 200    # number of distractor absent\nbeta_0     = 650    # Intercept\nbeta_1     = 30     # effect of distractor presence\ntau_0      = 80     # by-subject random intercept sd\ntau_1      = 15     # by-subject random slope sd\nrho        = 0.35   # correlation between intercept and slope\nsigma      = 175    # residual nose\n\nGenerate trials with their fixed effects:\n\n# simulate a sample of items\n# total number of items = n_ingroup + n_outgroup\n\nitems = (pd\n  .DataFrame({\n    'distractor' : np.repeat(['absent', 'present'], [n_absent, n_present])})\n  .assign(X_i=lambda x: np.where(x[\"distractor\"] == 'present', 1, 0)))\n\nitems.describe()\n\n\n\n\n\n\n\n\nX_i\n\n\n\n\ncount\n400.000000\n\n\nmean\n0.500000\n\n\nstd\n0.500626\n\n\nmin\n0.000000\n\n\n25%\n0.000000\n\n\n50%\n0.500000\n\n\n75%\n1.000000\n\n\nmax\n1.000000\n\n\n\n\n\n\n\nAnd generate participants with their random intercepts and slopes:\n\n# simulate a sample of subjects\n\n# calculate random intercept / random slope covariance\ncovar = rho * tau_0 * tau_1\n\n# put values into variance-covariance matrix\ncov_mx = np.array([\n  [tau_0**2, covar], \n  [covar, tau_1**2]\n])\n\n# generate the by-subject random effects\nsubject_rfx = np.random.multivariate_normal(mean = [0, 0], cov = cov_mx, size = n_subj)\n\n# combine with subject IDs\nsubjects = pd.DataFrame({\n  'subj_id': range(n_subj),\n  'T_0s': subject_rfx[:,0],\n  'T_1s': subject_rfx[:,1]\n})\n\nsubjects.describe()\n\n\n\n\n\n\n\n\nsubj_id\nT_0s\nT_1s\n\n\n\n\ncount\n10.00000\n10.000000\n10.000000\n\n\nmean\n4.50000\n-17.271127\n-0.859632\n\n\nstd\n3.02765\n74.746471\n13.287269\n\n\nmin\n0.00000\n-139.318368\n-23.359813\n\n\n25%\n2.25000\n-63.154617\n-7.704764\n\n\n50%\n4.50000\n-26.488825\n-3.103008\n\n\n75%\n6.75000\n37.953663\n3.729326\n\n\nmax\n9.00000\n96.445480\n23.913242\n\n\n\n\n\n\n\nNow combine and add residual noise to create a complete dataframe:\n\n#cross items and subjects, add noise\nitems['key'] = 1\nsubjects['key'] = 1\ntrials = (pd\n  .merge(items,subjects, on='key')\n  .drop(\"key\",axis=1)\n  .assign(e_si = lambda x: np.random.normal(scale=sigma,size=len(x))))\n\n\n# calculate the response variable\ndat_sim = (trials\n  .assign(RT = lambda x: beta_0 + x.T_0s + (beta_1 + x.T_1s) * x.X_i + x.e_si)\n  .filter(items=['subj_id', 'distractor', 'X_i', 'RT']))\n\ndat_sim.head(10)\n\n\n\n\n\n\n\n\nsubj_id\ndistractor\nX_i\nRT\n\n\n\n\n0\n0\nabsent\n0\n832.331252\n\n\n1\n1\nabsent\n0\n580.917390\n\n\n2\n2\nabsent\n0\n498.343339\n\n\n3\n3\nabsent\n0\n811.465070\n\n\n4\n4\nabsent\n0\n425.125071\n\n\n5\n5\nabsent\n0\n669.570885\n\n\n6\n6\nabsent\n0\n758.727503\n\n\n7\n7\nabsent\n0\n946.781381\n\n\n8\n8\nabsent\n0\n776.457095\n\n\n9\n9\nabsent\n0\n722.647844\n\n\n\n\n\n\n\nData generated! Does it look like we’d expect?\n\nsns.catplot(\n  data=dat_sim.astype({\"subj_id\": \"string\"}),\n  x=\"distractor\",\n  y=\"RT\",\n  hue=\"subj_id\",\n  kind=\"point\",\n  legend=False\n)\n\n\n\n\nLooks comparable to the original data! Now let’s fit a model to see if we recover the parameters:\n\nmd = smf.mixedlm(\"RT ~ distractor\", dat_sim, groups=dat_sim['subj_id'],  re_formula=\"~distractor\")\nmdf = md.fit()\nmdf.summary()\n\n\n\n\nModel:\nMixedLM\nDependent Variable:\nRT\n\n\nNo. Observations:\n4000\nMethod:\nREML\n\n\nNo. Groups:\n10\nScale:\n31160.2343\n\n\nMin. group size:\n400\nLog-Likelihood:\n-26383.8513\n\n\nMax. group size:\n400\nConverged:\nNo\n\n\nMean group size:\n400.0\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n635.249\n26.091\n24.348\n0.000\n584.112\n686.386\n\n\ndistractor[T.present]\n32.013\n5.923\n5.405\n0.000\n20.405\n43.622\n\n\nGroup Var\n6651.458\n17.101\n\n\n\n\n\n\nGroup x distractor[T.present] Cov\n-454.911\n3.631\n\n\n\n\n\n\ndistractor[T.present] Var\n39.172\n1.918\n\n\n\n\n\n\n\n\n\nGreat, our simulation works - we recover our ground truth for the different coefficients (allowing for differences due to noise and limited sample size). Now for a power analysis, we’d put the above in functions and run the code many times for a given combination of parameters. See below:\n\ndef my_sim_data(\n  n_subj     = 5,       # number of subjects\n  n_present  = 200,     # number of distractor present trials\n  n_absent   = 200,     # number of distractor absent\n  beta_0     = 650,     # Intercept\n  beta_1     = 30,      # effect of distractor presence\n  tau_0      = 80,      # by-subject random intercept sd\n  tau_1      = 15,      # by-subject random slope sd\n  rho        = 0.35,    # correlation between intercept and slope\n  sigma      = 175      # residual noise\n  ):\n  \n\n  # simulate a sample of items\n  # total number of items = n_ingroup + n_outgroup\n  items = (pd.DataFrame({\n  'distractor' : np.repeat(['absent', 'present'], [n_absent, n_present])\n})\n  .assign(X_i=lambda x: np.where(x[\"distractor\"] == 'present', 1, 0)))\n\n\n  # simulate a sample of subjects\n\n  # calculate random intercept / random slope covariance\n  covar = rho * tau_0 * tau_1\n\n  # put values into variance-covariance matrix\n  cov_mx = np.array([\n    [tau_0**2, covar], \n    [covar, tau_1**2]\n  ])\n\n  # generate the by-subject random effects\n  subject_rfx = np.random.multivariate_normal(mean = [0, 0], cov = cov_mx, size = n_subj)\n\n  # combine with subject IDs\n  subjects = pd.DataFrame({\n    'subj_id': range(n_subj),\n    'T_0s': subject_rfx[:,0],\n    'T_1s': subject_rfx[:,1]\n  })\n\n  #cross items and subjects, add noise\n  items['key'] = 1\n  subjects['key'] = 1\n  trials = (pd\n    .merge(items,subjects, on='key').drop(\"key\",axis=1)\n    .assign(e_si = lambda x: np.random.normal(scale=sigma,size=len(x))))\n\n\n  # calculate the response variable\n  dat_sim = (trials\n    .assign(RT = lambda x: beta_0 + x.T_0s + (beta_1 + x.T_1s) * x.X_i + x.e_si)\n    .filter(items=['subj_id', 'distractor', 'X_i', 'RT']))\n\n  return dat_sim\n\nThe above function simulates data. The function below combines it with a model fit so we have a function that can be repeatedly called during our power analysis.\n\ndef single_run(filename = None, *args, **kwargs):\n  dat_sim = my_sim_data(*args, **kwargs)\n  with warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(\"ignore\")\n    md = smf.mixedlm(\"RT ~ distractor\", dat_sim, groups=dat_sim['subj_id'], re_formula=\"~distractor\")\n    md_null = smf.mixedlm(\"RT ~ 1\", dat_sim, groups=dat_sim['subj_id'], re_formula=\"~distractor\")\n\n    # As above, we use the LRT method to obtain a better estimate of the coefficient's significance. This does mean we're fitting 2 models every sim - consider switching to the default p-values (so removing the null model definition, fit, the lrt calculation, and the line that replaces the p-value) if this takes too long (psst, or switch to Julia)\n    mod_sim = md.fit(reml=False)\n    mod_sim_null = md_null.fit(reml=False)\n    lrt = 1 - chi2.cdf(-2*(mod_sim_null.llf - mod_sim.llf),1)\n  sim_results = (mod_sim\n    .summary()\n    .tables[1]\n    .assign(**kwargs))\n  sim_results = sim_results.apply(pd.to_numeric, errors='coerce')\n  sim_results.index.rename('i',inplace=True)\n  sim_results.at[1,'P&gt;|z|'] = lrt\n\n  if not filename == None:\n    hdr = not os.path.isfile(filename)\n    sim_results.to_csv(filename, mode='a',header=hdr)\n  \n  return sim_results\n\nNow let’s run our sensitivity analysis - we will run our simulation many times (100 times here for speed, but aim for more, like 1000+) for each combination of parameters, and record how often the fixed effect estimates reach significance:\n\nnreps = 100\n\nparams = ParameterGrid({\n  'n_subj'     : [7],       # number of subjects\n  'n_present'  : [150],     # number of distractor present trials\n  'n_absent'   : [150],     # number of distractor absent\n  'beta_0'     : [650],     # Intercept\n  'beta_1'     : [30],      # effect of distractor presence\n  'tau_0'      : [80],      # by-subject random intercept sd\n  'tau_1'      : [15],      # by-subject random slope sd\n  'rho'        : [0.35],    # correlation between intercept and slope\n  'sigma'      : [175]      # residual (standard deviation)\n})\n\nsims = pd.concat([single_run(**param) for param in params for i in range(nreps)])\n\n\n  \nalpha = 0.05\n\n(sims\n  .assign(power = sims['P&gt;|z|'] &lt; alpha)\n  .query('i==\"Intercept\" or i==\"distractor[T.present]\"')\n  .groupby(['i'])\n  .agg(\n      mean_estimate = ('Coef.','mean'),\n      mean_se = ('Coef.', 'sem'),\n      power = ('power', 'mean')))\n\n\n\n\n\n\n\n\nmean_estimate\nmean_se\npower\n\n\ni\n\n\n\n\n\n\n\nIntercept\n653.43452\n3.386895\n1.00\n\n\ndistractor[T.present]\n30.13999\n0.924530\n0.86\n\n\n\n\n\n\n\nIf we want to run our sensitivity analysis across a given parameter space, we’ll have to map the function single_run to generate data across this space, for example, over a varying number of participants:\n\nfilename1 = \"sens_py.csv\"\nnreps = 1000\n\nparams = ParameterGrid({\n  'n_subj'     : range(2,15), # number of subjects\n  'n_present'  : [150],       # number of distractor present trials\n  'n_absent'   : [150],       # number of distractor absent\n  'beta_0'     : [650],       # Intercept\n  'beta_1'     : [30],        # effect of category\n  'tau_0'      : [80],        # by-subject random intercept sd\n  'tau_1'      : [15],        # by-subject random slope sd\n  'rho'        : [0.35],      # correlation between intercept and slope\n  'sigma'      : [175]        # residual (standard deviation)\n})\n\n\n\nif not os.path.isfile(filename1):\n  # run a simulation for each row of params\n  # and save to a file on each rep\n  sims1 = pd.concat([single_run(**param, filename=filename1) for param in params for i in range(nreps)])\n\nNote that the above could obviously also be run over other dimensions of our parameter space, e.g. for different estimates of the fixed effects, amount of noise, number of trials, etc. etc., by changing the params list. How did we do? Let’s take a look at our power curve.\n\nsims1 = pd.read_csv('sens_py.csv')\n\npower1 = (sims1.assign(power = sims1['P&gt;|z|'] &lt; alpha)\n  .query('i==\"distractor[T.present]\"')\\\n  .groupby(['n_subj'])\\\n  .agg(\n    mean_estimate = ('Coef.','mean'),\n    mean_se = ('Coef.', 'sem'),\n    power = ('power', 'mean')))\n\n\nsns.regplot(x=power1.index, y=power1[\"power\"],lowess=True)\nplt.axhline(y=.8)\n\n&lt;matplotlib.lines.Line2D at 0x23ea5cb61c0&gt;\n\n\n\n\n\nOur power analysis has determined that, with the parameters established above, we need ~8 or more participants to reliably detect an effect!\nThe code used above is specific to power analysis for mixed models, but the approach generalises to other methods too, of course! The above code can easily be wrangled to handle different model types (simply change the model definition in single_run and make sure to capture the right parameters), and even Bayesian approaches. (For a thorough example of doing power analysis with Bayesian methods and the awesome bayesian regression package brms, see Kurz (2021).)\nEven if the above code is spaghetti to you (Perhaps you prefer R? or Julia?), I hope you will take away a few things from this tutorial:\n\nPower analysis is nothing more than testing whether we can recover the parameters of a hypothesised data-generating process reliably using our statistical test of choice.\nWe can determine the parameters for such a data-generating process in the same way we formulate hypotheses (and indeed, in some ways these two things are one and the same): we use our knowledge, intuition, and previous work to inform our decision-making.\nIf you have a hypothetical data-generating process, you can simulate data by simply formalising that process as code and letting it simulate a dataset\nSimulation can help you answer questions about your statistical approach that are difficult to answer with other tools\n\n\nReferences\n\n\nAdam, K. C. S., Patel, T., Rangan, N., & Serences, J. T. (2021). Classic Visual Search Effects in an Additional Singleton Task: An Open Dataset. 4(1), 34. https://doi.org/10.5334/joc.182\n\n\nDeBruine, L. M., & Barr, D. J. (2020). Appendix 1c: Sensitivity Analysis. https://debruine.github.io/lmem_sim/articles/appendix1c_sensitivity.html.\n\n\nDeBruine, L. M., & Barr, D. J. (2021). Understanding Mixed-Effects Models Through Data Simulation. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920965119. https://doi.org/10.1177/2515245920965119\n\n\nKurz, A. S. (2021). Bayesian power analysis: Part I. Prepare to reject ‘\\(H_0\\)‘ with simulation. In A. Solomon Kurz. https://solomonkurz.netlify.app/blog/bayesian-power-analysis-part-i/.\n\n\nLakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence Testing for Psychological Research: A Tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963"
  },
  {
    "objectID": "index_old.html",
    "href": "index_old.html",
    "title": "Power Simulation in a Mixed Effects design using R",
    "section": "",
    "text": "Note\n\n\n\nThis is an older version of this page, which doesn’t use the faux package. I recommend using faux because it makes setting up simulations faster and easier, but I have left up this version for those who prefer not to install more packages. This version is also slightly more verbose during the simulation setup, which might aid understanding.\n\n\nIn this notebook we’ll go through a quick example of setting up a power analysis, using data from an existing, highly-powered study to make credible parameter estimates. The code for setting up a simulation is inspired by/shamelessly stolen from a great tutorial about this topic by DeBruine & Barr (2021) and Lisa DeBruine’s appendix on its application for sensitivity analysis DeBruine & Barr (2020). The aim of this tutorial is two-fold:\n\nTo demonstrate this approach for the most basic mixed model (using it only to deal with repeated measures - no nesting, no crossed random effects with stimuli types, etc.) which is a very common use of the technique for researchers in my immediate environment (visual attention research).\nTo translate this approach to different languages - although I love R and encourage everyone to use it for statistical analysis, Python remains in use by a sizeable number of researchers, and I would also like to introduce Julia as an alternative.\n\nBefore we do anything, let’s import all the packages we will need:\n\nlibrary(tidyverse) # Data wrangling, plotting and general awesomeness\nlibrary(lmerTest) # Mixed modeling using lme4 with better support for tests\nlibrary(broom.mixed) # To make pretty tables\nlibrary(knitr) # To print those pretty tables\n\nset.seed(90059)\n\nIn this example, we will make an estimate of the number of participants we need to replicate a simple and well-established experimental finding: The capture of attention by a colour singleton during visual search for a unique shape singleton. For this example, we are fortunate in that there are many studies of this effect for us to base our parameter estimates on. One recent example is a highly-powered study by Kirsten Adam from the Serences lab purpose-built to be used for sensitivity analysis. First let’s import the data for our specific case from the Adam et al. (2021) study, which is freely available in an OSF repository, and look at the data.\nNote that when previous data doesn’t exist (or even if it does, but you don’t trust that it’s sufficient to base your effect estimates on) there are alternative ways of determining such parameters, including formally determining a smallest effect size of interest Lakens et al. (2018).\nThe data we chose is from experiment 1c: variable colour singleton search. We are interested in the raw trial data, not the summary data (We are doing a mixed model after all, not an ANOVA) so we have to grab all the raw files and concatenate them.\n\ndf &lt;- list.files(\n  path = \"./Experiment_1c\",\n  full.names = T) %&gt;% \n  lapply(\n    read_csv,\n    col_types = cols(\n      gender = \"c\",\n      set_size = \"f\"\n    )\n  ) %&gt;% \n  bind_rows\n\nOnce it’s imported, we can take a look at our data, e.g., looking at subject means between the two conditions:\n\ndf %&gt;%\n    filter(\n        acc == 1,\n        set_size == 4\n    ) %&gt;%\n    mutate(rt = rt * 1000) %&gt;%\n    ggplot(\n        aes(\n            x = distractor,\n            y = rt,\n            color = as.factor(subject),\n            group = as.factor(subject)\n        )\n    ) +\n    guides(color = \"none\") +\n    stat_summary(\n        fun.data = \"mean_se\",\n        size = 1,\n        linewidth = 1\n    ) +\n    stat_summary(\n      fun = \"mean\",\n      geom=\"line\"\n    )+\n    theme_bw() +\n    ggtitle(\"Reaction time by participant\") +\n    xlab(\"Colour singleton\") +\n    ylab(\"Reaction time (ms)\") +\n    theme(text = element_text(size = 20))\n\n\n\n\nWe can clearly see typical atttentional capture effects in the data. Now that we have the data, let’s model it:\n\nd &lt;- df %&gt;% \n  filter(acc == 1,\n         set_size == 4) %&gt;% \n  mutate( rt = rt*1000)\n\n# Our model is simple: RT is dependent on distractor presence, with a random slope and intercept for each subject. More complex models are left as an exercise to the reader.\n\nm1 &lt;- lmer(rt ~ distractor + ( distractor | subject), data = d)\n\nkable(tidy(m1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\nstatistic\ndf\np.value\n\n\n\n\nfixed\nNA\n(Intercept)\n651.5000492\n16.66271\n39.099285\n22.99885\n0.0e+00\n\n\nfixed\nNA\ndistractorpresent\n30.8566430\n4.75268\n6.492473\n22.90957\n1.3e-06\n\n\nran_pars\nsubject\nsd__(Intercept)\n80.6315541\nNA\nNA\nNA\nNA\n\n\nran_pars\nsubject\ncor__(Intercept).distractorpresent\n0.3498972\nNA\nNA\nNA\nNA\n\n\nran_pars\nsubject\nsd__distractorpresent\n14.7695907\nNA\nNA\nNA\nNA\n\n\nran_pars\nResidual\nsd__Observation\n175.8140465\nNA\nNA\nNA\nNA\n\n\n\n\n\nThe above model rt ~ distractor + ( distractor | subject) is our putative data generating process, the parameters that we believe underly the generation of observed dependent variables, and the relationship between those parameters. The table shown above gives us parameter estimates for all fixed and random effects in the model. Now let’s plug those parameters into a simulation!\n\nn_subj     &lt;- 10    # number of subjects\nn_present  &lt;- 200   # number of distractor present trials\nn_absent   &lt;- 200   # number of distractor absent trials\nbeta_0     &lt;- 650   # Intercept\nbeta_1     &lt;- 30    # effect of distractor presence\ntau_0      &lt;- 80    # by-subject random intercept sd\ntau_1      &lt;- 15    # by-subject random slope sd\nrho        &lt;- 0.35  # correlation between intercept and slope\nsigma      &lt;- 175   # residual nose\n\nGenerate trials with their fixed effects:\n\n# simulate a sample of items\n# total number of items = n_ingroup + n_outgroup\n\nitems &lt;- data.frame(\n  distractor = rep(c(\"present\", \"absent\"), c(n_present, n_absent))\n)\n\n# effect-code distractor\nitems$X_i &lt;- recode(items$distractor, \"absent\" = 0, \"present\" = 1)\n\n# taking a peek at the items dataframe without changing it\nitems %&gt;% \n  group_by(distractor) %&gt;% \n  summarise(trial_count = n(), X_i = mean(X_i)) %&gt;% \n  kable(.)\n\n\n\n\ndistractor\ntrial_count\nX_i\n\n\n\n\nabsent\n200\n0\n\n\npresent\n200\n1\n\n\n\n\n\nAnd generate participants with their random intercepts and slopes:\n\n# simulate a sample of subjects\n\n# calculate random intercept / random slope covariance\ncovar &lt;- rho * tau_0 * tau_1\n\n# put values into variance-covariance matrix\ncov_mx  &lt;- matrix(\n  c(tau_0^2, covar,\n    covar,   tau_1^2),\n  nrow = 2, byrow = TRUE)\n\n# generate the by-subject random effects\nsubject_rfx &lt;- MASS::mvrnorm(n = n_subj,\n                             mu = c(T_0s = 0, T_1s = 0),\n                             Sigma = cov_mx)\n\n# combine with subject IDs\nsubjects &lt;- data.frame(subj_id = seq_len(n_subj),\n                       subject_rfx)\n\nkable(head(subjects))\n\n\n\n\nsubj_id\nT_0s\nT_1s\n\n\n\n\n1\n-89.290493\n5.816253\n\n\n2\n-148.337118\n-13.046963\n\n\n3\n12.117238\n14.610919\n\n\n4\n2.359166\n-1.880143\n\n\n5\n17.247824\n1.459213\n\n\n6\n-31.761044\n-20.142109\n\n\n\n\n\nNow combine and add residual noise to create a complete dataframe:\n\n# cross subjects and items; add an error term\n# nrow(.) is the number of rows in the table\ntrials &lt;- expand_grid(subjects, items)  %&gt;%\n  mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n  select(subj_id, distractor, X_i, everything())\n\n# calculate the response variable\ndat_sim &lt;- trials %&gt;%\n  mutate(RT = beta_0 + T_0s + (beta_1 + T_1s) * X_i + e_si) %&gt;%\n  select(subj_id, distractor, X_i, RT)\n\nkable(head(dat_sim))\n\n\n\n\nsubj_id\ndistractor\nX_i\nRT\n\n\n\n\n1\npresent\n1\n722.5261\n\n\n1\npresent\n1\n675.6957\n\n\n1\npresent\n1\n662.8946\n\n\n1\npresent\n1\n340.1661\n\n\n1\npresent\n1\n612.0086\n\n\n1\npresent\n1\n823.7334\n\n\n\n\n\nData generated! Does it look like we’d expect?\n\ndat_sim %&gt;% \n  ggplot(\n    aes(\n      x=distractor,\n      y=RT,\n      color=as.factor(subj_id),\n      group=as.factor(subj_id)\n    )) +\n  guides(color = \"none\") +\n  stat_summary(\n    fun.data = \"mean_se\",\n    size=1\n    ) +\n  theme_bw()+\n  ggtitle(\"Reaction time by participant (simulated)\")+\n  xlab(\"Colour singleton\")+\n  ylab(\"Reaction time (ms)\")+\n  theme(text=element_text(size=20))\n\n\n\n\nLooks comparable to the original data! Now let’s fit a model to see if we recover the parameters:\n\nm_sim &lt;- lmer(RT ~ distractor + (distractor | subj_id), dat_sim)\nkable(tidy(m_sim))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\nstatistic\ndf\np.value\n\n\n\n\nfixed\nNA\n(Intercept)\n636.1507193\n22.384871\n28.418780\n8.999582\n0.0000000\n\n\nfixed\nNA\ndistractorpresent\n28.4240022\n6.110523\n4.651648\n8.999334\n0.0011996\n\n\nran_pars\nsubj_id\nsd__(Intercept)\n69.7292857\nNA\nNA\nNA\nNA\n\n\nran_pars\nsubj_id\ncor__(Intercept).distractorpresent\n0.1328515\nNA\nNA\nNA\nNA\n\n\nran_pars\nsubj_id\nsd__distractorpresent\n8.7225522\nNA\nNA\nNA\nNA\n\n\nran_pars\nResidual\nsd__Observation\n172.4244621\nNA\nNA\nNA\nNA\n\n\n\n\n\nGreat, our simulation works - we recover our ground truth for the different coefficients (allowing for differences due to noise and limited sample size). Now for a power analysis, we’d put the above in functions and run the code many times for a given combination of parameters. See below:\n\nmy_sim_data &lt;- function(\n  \n  n_subj     = 5,       # number of subjects\n  n_present  = 200,     # number of distractor present trials\n  n_absent   = 200,     # number of distractor absent\n  beta_0     = 650,     # Intercept\n  beta_1     = 30,      # effect of distractor presence\n  tau_0      = 80,      # by-subject random intercept sd\n  tau_1      = 15,      # by-subject random slope sd\n  rho        = 0.35,    # correlation between intercept and slope\n  sigma      = 175      # residual variance\n  )\n  {\n\n# simulate a sample of items\n# total number of items = n_ingroup + n_outgroup\nitems &lt;- data.frame(\n  distractor = rep(c(\"present\", \"absent\"), c(n_present, n_absent))\n)\n\n# effect-code distractor presence\nitems$X_i &lt;- recode(items$distractor, \"absent\" = 0, \"present\" = 1)\n\n# simulate a sample of subjects\n\n# calculate random intercept / random slope covariance\ncovar &lt;- rho * tau_0 * tau_1\n\n# put values into variance-covariance matrix\ncov_mx  &lt;- matrix(\n  c(tau_0^2, covar,\n    covar,   tau_1^2),\n  nrow = 2, byrow = TRUE)\n\n# generate the by-subject random effects\nsubject_rfx &lt;- MASS::mvrnorm(n = n_subj,\n                             mu = c(T_0s = 0, T_1s = 0),\n                             Sigma = cov_mx)\n\n# combine with subject IDs\nsubjects &lt;- data.frame(subj_id = seq_len(n_subj),\n                       subject_rfx)\n\n# cross subject and items; add an error term\n# nrow(.) is the number of rows in the table\ntrials &lt;- expand_grid(subjects, items)  %&gt;%\n  mutate(e_si = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n  select(subj_id, distractor, X_i, everything())\n\n# calculate the response variable\ndat_sim &lt;- trials %&gt;%\n  mutate(RT = beta_0 + T_0s + (beta_1 + T_1s) * X_i + e_si) %&gt;%\n  select(subj_id, distractor, X_i, RT)\n}\n\nThe above function simulates data. The function below combines it with a model fit so we have a function that can be repeatedly called during our power analysis.\n\nsingle_run &lt;- function(filename = NULL, ...) {\n  # ... is a shortcut that forwards any arguments to my_sim_data()\n  dat_sim &lt;- my_sim_data(...)\n  \n  # run lmer and capture any warnings\n  ww &lt;- \"\"\n  suppressMessages(suppressWarnings(\n    mod_sim &lt;- withCallingHandlers({\n      lmer(RT ~ X_i + (1 + X_i | subj_id),\n           dat_sim, REML = FALSE)},\n      warning = function(w) { ww &lt;&lt;- w$message }\n    )\n  ))\n  \n  # get results table and add rep number and any warnings\n  sim_results &lt;- broom.mixed::tidy(mod_sim) %&gt;%\n    mutate(warnings = ww)\n  \n  # add columns for the specified parameters\n  params &lt;- list(...)\n  for (name in names(params)) {\n    sim_results[name] &lt;- params[name]\n  }\n  \n  # append the results to a file if filename is set\n  if (!is.null(filename)) {\n    append &lt;- file.exists(filename) # append if the file exists\n    write_csv(sim_results, filename, append = append)\n  }\n  \n  sim_results\n}\n\nNow let’s run our sensitivity analysis - we will run our simulation 1000 times for each combination of parameters, and record how often the fixed effect estimates reach significance:\n\nnreps &lt;- 1000\n\nparams &lt;- crossing(\n  rep        = 1:nreps,   # number of runs\n  n_subj     = 10,        # number of subjects\n  n_present  = 150,       # number of distractor present trials\n  n_absent   = 150,       # number of distractor absent\n  beta_0     = 650,       # Intercept\n  beta_1     = 30,        # effect of distractor presence\n  tau_0      = 80,        # by-subject random intercept sd\n  tau_1      = 15,        # by-subject random slope sd\n  rho        = 0.35,      # correlation between intercept and slope\n  sigma      = 175        # residual (standard deviation)\n) %&gt;%\n  select(-rep)\n  \nsims &lt;- purrr::pmap_df(params,single_run,filename=NULL)\n\n# calculate mean estimates and power for specified alpha\nalpha &lt;- 0.05\nsims %&gt;% \n  filter(effect == \"fixed\") %&gt;%\n  group_by(term) %&gt;%\n  summarize(\n    mean_estimate = mean(estimate),\n    mean_se = mean(std.error),\n    power = mean(p.value &lt; alpha),\n    .groups = \"drop\"\n  )\n\n# A tibble: 2 × 4\n  term        mean_estimate mean_se power\n  &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)         650.    23.9  1    \n2 X_i                  29.8    7.87 0.942\n\n\nIf we want to run our sensitivity analysis across a given parameter space, we’ll have to map the function single_run to generate data across this space, for example, over a varying number of participants:\n\nfilename1 &lt;- \"sens1.csv\"\nnreps &lt;- 1000 # number of replications per parameter combo\n\nparams &lt;- crossing(\n  rep        = 1:nreps,     # repeats each combo nreps times\n  n_subj     = seq(2, 15),  # number of subjects\n  n_present  = 150,         # number of distractor present trials\n  n_absent   = 150,         # number of distractor absent\n  beta_0     = 650,         # Intercept\n  beta_1     = 30,          # effect of distractor presence\n  tau_0      = 80,          # by-subject random intercept sd\n  tau_1      = 15,          # by-subject random slope sd\n  rho        = 0.35,        # correlation between intercept and slope\n  sigma      = 175          # residual (standard deviation)\n) %&gt;%\n  select(-rep)\n\nif (!file.exists(filename1)) {\n  # run a simulation for each row of params\n  # and save to a file on each rep\n  sims1 &lt;- purrr::pmap_df(params, single_run, filename = filename1)\n}\n\nNote that the above could obviously also be run over other dimensions of our parameter space, e.g. for different estimates of the fixed effects, amount of noise, number of trials, etc. etc., by changing the params list. How did we do? Let’s take a look at our power curve.\n\n# read saved simulation data\n# NB: col_types is set for warnings in case \n#     the first 1000 rows don't have any\nct &lt;- cols(warnings = col_character(),\n           # makes sure plots display in this order\n           group = col_factor(ordered = TRUE),\n           term = col_factor(ordered = TRUE))\nsims1 &lt;- read_csv(filename1, col_types = ct)\n\npower1 &lt;- sims1 %&gt;% \n  filter(effect == \"fixed\", term == \"X_i\") %&gt;%\n  group_by(n_subj) %&gt;%\n  summarise(\n    mean_estimate = mean(estimate),\n    mean_se = mean(std.error),\n    power = mean(p.value &lt; alpha),\n    .groups = \"drop\"\n  ) \n\npower1 %&gt;%\n  ggplot(aes(n_subj, power)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  ylim(0, 1) +\n  geom_hline(yintercept=0.8,linetype=\"dashed\")+\n  scale_x_continuous(name = \"Effect of number of participants\") +\n  ggtitle(\"Power for designs varying in sample size\") +\n  theme_bw()\n\n\n\n\nOur power analysis has determined that, with the parameters established above, we need ~8 or more participants to reliably detect an effect!\nThe code used above is specific to power analysis for mixed models, but the approach generalises to other methods too, of course! The above code can easily be wrangled to handle different model types (simply change the model definition in single_run and make sure to capture the right parameters), and even Bayesian approaches. (For a thorough example of doing power analysis with Bayesian methods and the awesome bayesian regression package brms, see Kurz (2021).)\nEven if the above code is spaghetti to you (I was originally planning on also converting it to python/matlab, but there are only so many hours in the dayclick here for a python version or here for a julia version), I hope you will take away a few things from this tutorial:\n\nPower analysis is nothing more than testing whether we can recover the parameters of a hypothesised data-generating process reliably using our statistical test of choice.\nWe can determine the parameters for such a data-generating process in the same way we formulate hypotheses (and indeed, in some ways these two things are one and the same): we use our knowledge, intuition, and previous work to inform our decision-making.\nIf you have a hypothetical data-generating process, you can simulate data by simply formalising that process as code and letting it simulate a dataset\nSimulation can help you answer questions about your statistical approach that are difficult to answer with other tools\n\n\nReferences\n\n\nAdam, K. C. S., Patel, T., Rangan, N., & Serences, J. T. (2021). Classic Visual Search Effects in an Additional Singleton Task: An Open Dataset. 4(1), 34. https://doi.org/10.5334/joc.182\n\n\nDeBruine, L. M., & Barr, D. J. (2020). Appendix 1c: Sensitivity Analysis. https://debruine.github.io/lmem_sim/articles/appendix1c_sensitivity.html.\n\n\nDeBruine, L. M., & Barr, D. J. (2021). Understanding Mixed-Effects Models Through Data Simulation. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920965119. https://doi.org/10.1177/2515245920965119\n\n\nKurz, A. S. (2021). Bayesian power analysis: Part I. Prepare to reject ‘\\(H_0\\)‘ with simulation. In A. Solomon Kurz. https://solomonkurz.netlify.app/blog/bayesian-power-analysis-part-i/.\n\n\nLakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence Testing for Psychological Research: A Tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Power Simulation in a Mixed Effects design using R",
    "section": "",
    "text": "In this notebook we’ll go through a quick example of setting up a power analysis, using data from an existing, highly-powered study to make credible parameter estimates. The code for setting up a simulation is inspired by/shamelessly stolen from a great tutorial about this topic by DeBruine & Barr (2021) and Lisa DeBruine’s appendix on its application for sensitivity analysis DeBruine & Barr (2020). The aim of this tutorial is two-fold:\n\nTo demonstrate this approach for the most basic mixed model (using it only to deal with repeated measures - no nesting, no crossed random effects with stimuli types, etc.) which is a very common use of the technique for researchers in my immediate environment (visual attention research).\nTo translate this approach to different languages - although I love R and encourage everyone to use it for statistical analysis, Python remains in use by a sizeable number of researchers, and I would also like to introduce Julia as an alternative.\n\nBefore we do anything, let’s import all the packages we will need:\n\nlibrary(tidyverse) # Data wrangling, plotting and general awesomeness\nlibrary(lmerTest) # Mixed modeling using lme4 with better support for tests\nlibrary(broom.mixed) # To make pretty tables\nlibrary(knitr) # To print those pretty tables\nlibrary(faux) # Much easier random effects simulation\n\nset.seed(90059)\n\nIn this example, we will make an estimate of the number of participants we need to replicate a simple and well-established experimental finding: The capture of attention by a colour singleton during visual search for a unique shape singleton. For this example, we are fortunate in that there are many studies of this effect for us to base our parameter estimates on. One recent example is a highly-powered study by Kirsten Adam from the Serences lab purpose-built to be used for sensitivity analysis. First let’s import the data for our specific case from the Adam et al. (2021) study, which is freely available in an OSF repository, and look at the data.\nNote that when previous data doesn’t exist (or even if it does, but you don’t trust that it’s sufficient to base your effect estimates on) there are alternative ways of determining such parameters, including formally determining a smallest effect size of interest Lakens et al. (2018).\nThe data we chose is from experiment 1c: variable colour singleton search. We are interested in the raw trial data, not the summary data (We are doing a mixed model after all, not an ANOVA) so we have to grab all the raw files and concatenate them.\n\ndf &lt;- list.files(\n    path = \"./Experiment_1c\",\n    full.names = T\n) %&gt;%\n    lapply(\n        read_csv,\n        col_types = cols(\n            gender = \"c\",\n            set_size = \"f\"\n        )\n    ) %&gt;%\n    bind_rows()\n\nOnce it’s imported, we can take a look at our data, e.g., looking at subject means between the two conditions:\n\ndf %&gt;%\n    filter(\n        acc == 1,\n        set_size == 4\n    ) %&gt;%\n    mutate(rt = rt * 1000) %&gt;%\n    ggplot(\n        aes(\n            x = distractor,\n            y = rt,\n            color = as.factor(subject),\n            group = as.factor(subject)\n        )\n    ) +\n    guides(color = \"none\") +\n    stat_summary(\n        fun.data = \"mean_se\",\n        size = 1,\n        linewidth = 1\n    ) +\n    stat_summary(\n      fun = \"mean\",\n      geom=\"line\",\n      linewidth=1\n    )+\n    theme_bw() +\n    ggtitle(\"Reaction time by participant\") +\n    xlab(\"Colour singleton\") +\n    ylab(\"Reaction time (ms)\") +\n    theme(text = element_text(size = 20))\n\n\n\n\nWe can clearly see typical atttentional capture effects in the data. Now that we have the data, let’s model it:\n\nd &lt;- df %&gt;%\n    filter(\n        acc == 1,\n        set_size == 4\n    ) %&gt;%\n    mutate(rt = rt * 1000)\n\n# Our model is simple: RT is dependent on distractor presence, with a random slope and intercept for each subject. More complex models are left as an exercise to the reader.\n\nm1 &lt;- lmer(rt ~ distractor + (distractor | subject), data = d)\n\nkable(tidy(m1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\nstatistic\ndf\np.value\n\n\n\n\nfixed\nNA\n(Intercept)\n651.5000492\n16.66271\n39.099285\n22.99885\n0.0e+00\n\n\nfixed\nNA\ndistractorpresent\n30.8566430\n4.75268\n6.492473\n22.90957\n1.3e-06\n\n\nran_pars\nsubject\nsd__(Intercept)\n80.6315541\nNA\nNA\nNA\nNA\n\n\nran_pars\nsubject\ncor__(Intercept).distractorpresent\n0.3498972\nNA\nNA\nNA\nNA\n\n\nran_pars\nsubject\nsd__distractorpresent\n14.7695907\nNA\nNA\nNA\nNA\n\n\nran_pars\nResidual\nsd__Observation\n175.8140465\nNA\nNA\nNA\nNA\n\n\n\n\n\nThe above model rt ~ distractor + ( distractor | subject) is our putative data generating process, the parameters that we believe underly the generation of observed dependent variables, and the relationship between those parameters. The table shown above gives us parameter estimates for all fixed and random effects in the model. Now let’s plug those parameters into a simulation!\n\nn_subj &lt;- 10      # number of subjects\nn_present &lt;- 200  # number of trials with a singleton present\nn_absent &lt;- 200   # number of trials without a singleton\nbeta_0 &lt;- 650     # grand mean\nbeta_1 &lt;- 30      # effect of distractor presence\ntau_0 &lt;- 80       # by-subject random intercept sd\ntau_1 &lt;- 15       # by-subject random slope sd\nrho &lt;- 0.35       # correlation between intercept and slope\nsigma &lt;- 175      # residual nose\n\n\n# Generate a dataframe with one column per subject\ndat_sim &lt;- add_random(subj = n_subj) %&gt;%\n    # Each subject does both types of trials\n    add_within(\"subj\", singleton = c(\"absent\", \"present\")) %&gt;%\n    # Because these trials are interchangeable, we expand the dataframe to account for trial numbers using the uncount trick\n    add_between(\"singleton\", trial_number=c(n_absent, n_present)) %&gt;%\n    uncount(trial_number) %&gt;% \n    # Give each participant a random intercept and slope\n    add_ranef(\"subj\", T0s = tau_0, T1s = tau_1, .cors = rho) %&gt;%\n    # Account for residual variance with the sigma term - random noise on each trial\n    mutate(sigma = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n    # Contrast code singleton so we can do the little summation at the bottom\n    add_contrast(\"singleton\", \"treatment\", colnames = \"s\") %&gt;%\n    # Give each trial the right mix of Intercept, slope, random effects, and residual noise\n    mutate(rt = beta_0 + T0s + (beta_1 + T1s) * s + sigma)\n\nData generated! Does it look like we’d expect?\n\ndat_sim %&gt;% \n    ggplot(\n        aes(\n            x = singleton,\n            y = rt,\n            color = subj,\n            group = subj\n        )\n    ) +\n    guides(color = \"none\") +\n    stat_summary(\n        fun.data = \"mean_se\",\n        size = 1,\n        linewidth = 1\n    ) +\n    stat_summary(\n      fun = \"mean\",\n      geom=\"line\",\n      linewidth=1\n    )+\n    theme_bw() +\n  ggtitle(\"Reaction time by participant (simulated)\")+\n  xlab(\"Colour singleton\")+\n  ylab(\"Reaction time (ms)\")+\n  theme(text=element_text(size=20))\n\n\n\n\nLooks comparable to the original data! Now let’s fit a model to see if we recover the parameters:\n\nm_sim &lt;- lmer(rt ~ singleton + (singleton | subj), dat_sim)\nkable(tidy(m_sim))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\nstatistic\ndf\np.value\n\n\n\n\nfixed\nNA\n(Intercept)\n633.7980977\n21.010593\n30.165645\n9.001584\n0.0000000\n\n\nfixed\nNA\nsingleton.present-absent\n33.1292455\n7.275868\n4.553305\n9.000460\n0.0013792\n\n\nran_pars\nsubj\nsd__(Intercept)\n65.3130861\nNA\nNA\nNA\nNA\n\n\nran_pars\nsubj\ncor__(Intercept).singleton.present-absent\n0.6406591\nNA\nNA\nNA\nNA\n\n\nran_pars\nsubj\nsd__singleton.present-absent\n15.2341893\nNA\nNA\nNA\nNA\n\n\nran_pars\nResidual\nsd__Observation\n172.4244744\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nmy_sim_data &lt;- function(\n    n_subj = 5, # number of subjects\n    n_absent = 200, # number of trials per condition\n    n_present = 200,\n    beta_0 = 650, # grand mean\n    beta_1 = 30, # effect of distractor presence\n    tau_0 = 80, # by-subject random intercept sd\n    tau_1 = 15, # by-subject random slope sd\n    rho = 0.35, # correlation between intercept and slope\n    sigma = 175) {\n    dat_sim &lt;- add_random(subj = n_subj) %&gt;%\n    add_within(\"subj\", singleton = c(\"absent\", \"present\")) %&gt;%\n    add_between(\"singleton\", trial_number=c(n_absent, n_present)) %&gt;%\n    uncount(trial_number) %&gt;% \n    add_ranef(\"subj\", T0s = tau_0, T1s = tau_1, .cors = rho) %&gt;%\n    mutate(sigma = rnorm(nrow(.), mean = 0, sd = sigma)) %&gt;%\n    add_contrast(\"singleton\", \"treatment\", colnames = \"s\") %&gt;%\n    mutate(rt = beta_0 + T0s + (beta_1 + T1s) * s + sigma)\n\n    dat_sim\n}\n\nThe above function simulates data. The function below combines it with a model fit so we have a function that can be repeatedly called during our power analysis.\n\nsingle_run &lt;- function(filename = NULL, ...) {\n    dat_sim &lt;- my_sim_data(...)\n      # run lmer and capture any warnings\n  ww &lt;- \"\"\n  suppressMessages(suppressWarnings(\n    mod_sim &lt;- withCallingHandlers({\n      lmer(rt ~ singleton + (singleton | subj),\n           dat_sim, REML = FALSE)},\n      warning = function(w) { ww &lt;&lt;- w$message }\n    )\n  ))\n  \n  # get results table and add rep number and any warnings\n  sim_results &lt;- broom.mixed::tidy(mod_sim) %&gt;%\n    mutate(warnings = ww)\n  \n  # add columns for the specified parameters\n  params &lt;- list(...)\n  for (name in names(params)) {\n    sim_results[name] &lt;- params[name]\n  }\n\n  # append the results to a file if filename is set\n  if (!is.null(filename)) {\n    append &lt;- file.exists(filename) # append if the file exists\n    write_csv(sim_results, filename, append = append)\n  }\n  \n  sim_results\n}\n\nNow let’s run our sensitivity analysis - we will run our simulation 1000 times for each combination of parameters, and record how often the fixed effect estimates reach significance:\n\nnreps &lt;- 1000\n\nparams &lt;- crossing(\n  rep        = 1:nreps,   # number of runs\n  n_subj     = 10,        # number of subjects\n  n_absent   = 150,       # number of trials per condition\n  n_present  = 150,\n  beta_0     = 650,       # grand mean\n  beta_1     = 30,        # effect of distractor presence\n  tau_0      = 80,        # by-subject random intercept sd\n  tau_1      = 15,        # by-subject random slope sd\n  rho        = 0.35,      # correlation between intercept and slope\n  sigma      = 175        # residual (standard deviation)\n) %&gt;%\n  select(-rep)\n  \nsims &lt;- purrr::pmap_df(params,single_run,filename=NULL)\n\n# calculate mean estimates and power for specified alpha\nalpha &lt;- 0.05\nsims %&gt;% \n  filter(effect == \"fixed\") %&gt;%\n  group_by(term) %&gt;%\n  summarize(\n    mean_estimate = mean(estimate),\n    mean_se = mean(std.error),\n    power = mean(p.value &lt; alpha),\n    .groups = \"drop\"\n  )\n\n# A tibble: 2 × 4\n  term                     mean_estimate mean_se power\n  &lt;chr&gt;                            &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)                      650.    23.9  1    \n2 singleton.present-absent          29.9    7.81 0.929\n\n\nIf we want to run our sensitivity analysis across a given parameter space, we’ll have to map the function single_run to generate data across this space, for example, over a varying number of participants:\n\nfilename1 &lt;- \"sens_faux.csv\"\nnreps &lt;- 1000                # number of replications per parameter combo\n\nparams &lt;- crossing(\n  rep         = 1:nreps,     # repeats each combo nreps times\n  n_subj      = seq(2, 15),  # number of subjects\n  n_present   = 150,         # number of distractor present trials\n  n_absent    = 150,         # number of distractor absent trials\n  beta_0      = 650,         # Intercept\n  beta_1      = 30,          # effect of distractor presence\n  tau_0       = 80,          # by-subject random intercept sd\n  tau_1       = 15,          # by-subject random slope sd\n  rho         = 0.35,        # correlation between intercept and slope\n  sigma       = 175          # residual (standard deviation)\n) %&gt;%\n  select(-rep)\n\nif (!file.exists(filename1)) {\n  # run a simulation for each row of params\n  # and save to a file on each rep\n  sims1 &lt;- purrr::pmap_df(params, single_run, filename = filename1)\n}\n\nNote that the above could obviously also be run over other dimensions of our parameter space, e.g. for different estimates of the fixed effects, amount of noise, number of trials, etc. etc., by changing the params list. How did we do? Let’s take a look at our power curve.\n\n# read saved simulation data\n# NB: col_types is set for warnings in case \n#     the first 1000 rows don't have any\nct &lt;- cols(warnings = col_character(),\n           # makes sure plots display in this order\n           group = col_factor(ordered = TRUE),\n           term = col_factor(ordered = TRUE))\nsims1 &lt;- read_csv(filename1, col_types = ct)\n\npower1 &lt;- sims1 %&gt;% \n  filter(effect == \"fixed\", term == \"singleton.present-absent\") %&gt;%\n  group_by(n_subj) %&gt;%\n  summarise(\n    mean_estimate = mean(estimate),\n    mean_se = mean(std.error),\n    power = mean(p.value &lt; alpha),\n    .groups = \"drop\"\n  ) \n\npower1 %&gt;%\n  ggplot(aes(n_subj, power)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  ylim(0, 1) +\n  geom_hline(yintercept=0.8,linetype=\"dashed\")+\n  scale_x_continuous(name = \"Effect of number of participants\") +\n  ggtitle(\"Power for designs varying in sample size\") +\n  theme_bw()\n\n\n\n\nOur power analysis has determined that, with the parameters established above, we need ~8 or more participants to reliably detect an effect!\nThe code used above is specific to power analysis for mixed models, but the approach generalises to other methods too, of course! The above code can easily be wrangled to handle different model types (simply change the model definition in single_run and make sure to capture the right parameters), and even Bayesian approaches. (For a thorough example of doing power analysis with Bayesian methods and the awesome bayesian regression package brms, see Kurz (2021).)\nEven if the above code is spaghetti to you (I was originally planning on also converting it to python/matlab, but there are only so many hours in the dayclick here for a python version or here for a julia version), I hope you will take away a few things from this tutorial:\n\nPower analysis is nothing more than testing whether we can recover the parameters of a hypothesised data-generating process reliably using our statistical test of choice.\nWe can determine the parameters for such a data-generating process in the same way we formulate hypotheses (and indeed, in some ways these two things are one and the same): we use our knowledge, intuition, and previous work to inform our decision-making.\nIf you have a hypothetical data-generating process, you can simulate data by simply formalising that process as code and letting it simulate a dataset\nSimulation can help you answer questions about your statistical approach that are difficult to answer with other tools\n\n\nReferences\n\n\nAdam, K. C. S., Patel, T., Rangan, N., & Serences, J. T. (2021). Classic Visual Search Effects in an Additional Singleton Task: An Open Dataset. 4(1), 34. https://doi.org/10.5334/joc.182\n\n\nDeBruine, L. M., & Barr, D. J. (2020). Appendix 1c: Sensitivity Analysis. https://debruine.github.io/lmem_sim/articles/appendix1c_sensitivity.html.\n\n\nDeBruine, L. M., & Barr, D. J. (2021). Understanding Mixed-Effects Models Through Data Simulation. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920965119. https://doi.org/10.1177/2515245920965119\n\n\nKurz, A. S. (2021). Bayesian power analysis: Part I. Prepare to reject ‘\\(H_0\\)‘ with simulation. In A. Solomon Kurz. https://solomonkurz.netlify.app/blog/bayesian-power-analysis-part-i/.\n\n\nLakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence Testing for Psychological Research: A Tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963"
  },
  {
    "objectID": "julia.html",
    "href": "julia.html",
    "title": "Power Simulation in a Mixed Effects design using Julia",
    "section": "",
    "text": "In this notebook we’ll go through a quick example of setting up a power analysis, using data from an existing, highly-powered study to make credible parameter estimates. The code for setting up a simulation is inspired by/shamelessly stolen from a great tutorial about this topic by DeBruine & Barr (2021) and Lisa DeBruine’s appendix on its application for sensitivity analysis DeBruine & Barr (2020). The aim of this tutorial is two-fold:\n\nTo demonstrate this approach for the most basic mixed model (using it only to deal with repeated measures - no nesting, no crossed random effects with stimuli types, etc.) which is a very common use of the technique for researchers in my immediate environment (visual attention research).\nTo translate this approach to different languages - although I love R and encourage everyone to use it for statistical analysis, Python remains in use by a sizeable number of researchers, and I would also like to introduce Julia as an alternative.\n\nBefore we do anything, let’s import all the packages we will need:\n\nusing DataFrames\nusing DataFramesMeta\nusing CSV\nusing MixedModels\nusing Gadfly\nusing Statistics\nusing Distributions\nusing Random\n\nRandom.seed!(90059)\n\nTaskLocalRNG()\n\n\nIn this example, we will make an estimate of the number of participants we need to replicate a simple and well-established experimental finding: The capture of attention by a colour singleton during visual search for a unique shape singleton. For this example, we are fortunate in that there are many studies of this effect for us to base our parameter estimates on. One recent example is a highly-powered study by Kirsten Adam from the Serences lab purpose-built to be used for sensitivity analysis. First let’s import the data for our specific case from the Adam et al. (2021) study, which is freely available in an OSF repository, and look at the data.\nNote that when previous data doesn’t exist (or even if it does, but you don’t trust that it’s sufficient to base your effect estimates on) there are alternative ways of determining such parameters, including formally determining a smallest effect size of interest Lakens et al. (2018).\nThe data we chose is from experiment 1c: variable colour singleton search. We are interested in the raw trial data, not the summary data (We are doing a mixed model after all, not an ANOVA) so we have to grab all the raw files and concatenate them.\n\nall_files = readdir(\"Experiment_1c\", join = true)\ndfs = CSV.read.(all_files,DataFrame)\ndf = vcat(dfs...)\n\n38400×50 DataFrame38375 rows omitted\n\n\n\nRow\nexperiment\nsubject\nuniqueID\nblock\ntrial\ntrial_cumulative\nage\ngender\ncolor_cond_blocking\ncolor_history\nshape_type\nset_size\ndistractor\nrt\nacc\ntarget_orient\nresponse_key\nresponse_made\niti\ntarget_color\ntarget_item\ndistractor_item\ntarget_X\ntarget_Y\ndistractor_X\ndistractor_Y\nitem1_X\nitem1_Y\nitem2_X\nitem2_Y\nitem3_X\nitem3_Y\nitem4_X\nitem4_Y\nitem5_X\nitem5_Y\nitem6_X\nitem6_Y\nitem1_shape\nitem2_shape\nitem3_shape\nitem4_shape\nitem5_shape\nitem6_shape\nitem1_line\nitem2_line\nitem3_line\nitem4_line\nitem5_line\nitem6_line\n\n\n\nString3\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nAny\nString3\nString15\nString15\nInt64\nString7\nFloat64\nInt64\nString15\nString15\nInt64\nFloat64\nString7\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nString15\nString15\nString15\nString15\nString15\nString15\n\n\n\n\n1\n1c\n10\n100148\n1\n1\n1\n21\nM\nNA\nvariable\nhomogeneous\n5\nabsent\n1.09181\n1\nhorizontal\nhorizontal\n1\n0.929414\nred\n1\nNaN\n721.201\n842.519\nNaN\nNaN\n721.201\n842.519\n1006.3\n749.885\n1006.3\n450.115\n721.201\n357.481\n545.0\n600.0\n0.0\n0.0\n4\n0\n0\n0.0\n0.0\nNaN\nhorizontal\nhorizontal\nvertical\nhorizontal\nvertical\nnone\n\n\n2\n1c\n10\n100148\n1\n2\n2\n21\nM\nNA\nvariable\nhomogeneous\n6\nabsent\n1.42371\n1\nvertical\nvertical\n1\n0.705883\nred\n3\nNaN\n938.883\n386.139\nNaN\nNaN\n915.768\n827.207\n1054.65\n613.346\n938.883\n386.139\n684.232\n372.793\n545.349\n586.654\n661.117\n813.861\n0\n0\n4\n0.0\n0.0\n0.0\nvertical\nhorizontal\nvertical\nhorizontal\nhorizontal\nvertical\n\n\n3\n1c\n10\n100148\n1\n3\n3\n21\nM\nNA\nvariable\nhomogeneous\n5\npresent\n1.57339\n1\nvertical\nvertical\n1\n1.04706\nred\n3\n5.0\n853.017\n350.572\n610.498\n770.628\n903.718\n832.954\n1053.6\n573.345\n853.017\n350.572\n579.164\n472.5\n610.498\n770.628\n0.0\n0.0\n0\n0\n4\n0.0\n0.0\nNaN\nhorizontal\nvertical\nvertical\nvertical\nvertical\nnone\n\n\n4\n1c\n10\n100148\n1\n4\n4\n21\nM\nNA\nvariable\nhomogeneous\n5\npresent\n1.82955\n1\nhorizontal\nhorizontal\n1\n0.62353\nred\n2\n4.0\n1051.13\n555.72\n570.808\n488.215\n919.715\n825.152\n1051.13\n555.72\n835.489\n347.482\n570.808\n488.215\n622.862\n783.432\n0.0\n0.0\n0\n4\n0\n0.0\n0.0\nNaN\nhorizontal\nhorizontal\nvertical\nvertical\nhorizontal\nnone\n\n\n5\n1c\n10\n100148\n1\n5\n5\n21\nM\nNA\nvariable\nhomogeneous\n4\nabsent\n1.21572\n1\nvertical\nvertical\n1\n0.682353\ngreen\n3\nNaN\n746.983\n350.572\nNaN\nNaN\n853.017\n849.428\n1049.43\n546.983\n746.983\n350.572\n550.572\n653.017\n0.0\n0.0\n0.0\n0.0\n0\n0\n4\n0.0\nNaN\nNaN\nvertical\nvertical\nvertical\nvertical\nnone\nnone\n\n\n6\n1c\n10\n100148\n1\n6\n6\n21\nM\nNA\nvariable\nhomogeneous\n5\nabsent\n0.852387\n1\nvertical\nvertical\n1\n0.811766\ngreen\n4\nNaN\n800.0\n345.0\nNaN\nNaN\n650.115\n806.299\n949.885\n806.299\n1042.52\n521.201\n800.0\n345.0\n557.481\n521.201\n0.0\n0.0\n0\n0\n0\n4.0\n0.0\nNaN\nhorizontal\nhorizontal\nhorizontal\nvertical\nhorizontal\nnone\n\n\n7\n1c\n10\n100148\n1\n7\n7\n21\nM\nNA\nvariable\nhomogeneous\n4\nabsent\n0.622422\n1\nhorizontal\nhorizontal\n1\n0.811766\ngreen\n3\nNaN\n804.45\n345.039\nNaN\nNaN\n795.55\n854.961\n1054.96\n604.45\n804.45\n345.039\n545.039\n595.55\n0.0\n0.0\n0.0\n0.0\n0\n0\n4\n0.0\nNaN\nNaN\nvertical\nvertical\nhorizontal\nvertical\nnone\nnone\n\n\n8\n1c\n10\n100148\n1\n8\n8\n21\nM\nNA\nvariable\nhomogeneous\n4\nabsent\n1.52177\n1\nhorizontal\nhorizontal\n1\n0.752942\nred\n2\nNaN\n1029.19\n488.215\nNaN\nNaN\n911.785\n829.192\n1029.19\n488.215\n688.215\n370.808\n570.808\n711.785\n0.0\n0.0\n0.0\n0.0\n0\n4\n0\n0.0\nNaN\nNaN\nhorizontal\nhorizontal\nhorizontal\nhorizontal\nnone\nnone\n\n\n9\n1c\n10\n100148\n1\n9\n9\n21\nM\nNA\nvariable\nhomogeneous\n5\npresent\n0.662533\n1\nhorizontal\nhorizontal\n1\n0.705883\nred\n2\n3.0\n1045.12\n529.712\n808.899\n345.155\n942.594\n811.405\n1045.12\n529.712\n808.899\n345.155\n560.378\n512.785\n643.006\n800.943\n0.0\n0.0\n0\n4\n0\n0.0\n0.0\nNaN\nhorizontal\nhorizontal\nvertical\nhorizontal\nhorizontal\nnone\n\n\n10\n1c\n10\n100148\n1\n10\n10\n21\nM\nNA\nvariable\nhomogeneous\n3\nabsent\n0.733259\n1\nvertical\nvertical\n1\n0.658824\nred\n1\nNaN\n604.659\n763.911\nNaN\nNaN\n604.659\n763.911\n1039.62\n687.215\n755.72\n348.874\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4\n0\n0\nNaN\nNaN\nNaN\nvertical\nhorizontal\nvertical\nnone\nnone\nnone\n\n\n11\n1c\n10\n100148\n1\n11\n11\n21\nM\nNA\nvariable\nhomogeneous\n6\nabsent\n0.769892\n1\nhorizontal\nhorizontal\n1\n0.894119\ngreen\n3\nNaN\n1042.52\n521.201\nNaN\nNaN\n746.983\n849.428\n989.502\n770.628\n1042.52\n521.201\n853.017\n350.572\n610.498\n429.372\n557.481\n678.799\n0\n0\n4\n0.0\n0.0\n0.0\nvertical\nhorizontal\nhorizontal\nhorizontal\nvertical\nvertical\n\n\n12\n1c\n10\n100148\n1\n12\n12\n21\nM\nNA\nvariable\nhomogeneous\n5\npresent\n1.04978\n1\nvertical\nvertical\n1\n0.823531\ngreen\n5\n1.0\n549.685\n648.656\n768.923\n853.099\n768.923\n853.099\n1031.11\n707.768\n973.91\n413.505\n676.374\n376.972\n549.685\n648.656\n0.0\n0.0\n0\n0\n0\n0.0\n4.0\nNaN\nvertical\nvertical\nvertical\nvertical\nvertical\nnone\n\n\n13\n1c\n10\n100148\n1\n13\n13\n21\nM\nNA\nvariable\nhomogeneous\n4\nabsent\n0.662078\n1\nvertical\nvertical\n1\n0.752942\ngreen\n4\nNaN\n546.397\n626.655\nNaN\nNaN\n826.655\n853.603\n1053.6\n573.345\n773.345\n346.397\n546.397\n626.655\n0.0\n0.0\n0.0\n0.0\n0\n0\n0\n4.0\nNaN\nNaN\nhorizontal\nhorizontal\nhorizontal\nvertical\nnone\nnone\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n38389\n1c\n9\n100149\n25\n53\n1589\n18\nfalse\nNA\nvariable\nhomogeneous\n5\nabsent\n0.681741\n1\nhorizontal\nhorizontal\n1\n0.835291\ngreen\n2\nNaN\n1054.65\n613.346\nNaN\nNaN\n865.999\n846.311\n1054.65\n613.346\n891.384\n361.937\n601.828\n439.523\n586.139\n738.883\n0.0\n0.0\n0\n4\n0\n0.0\n0.0\nNaN\nhorizontal\nhorizontal\nhorizontal\nhorizontal\nhorizontal\nnone\n\n\n38390\n1c\n9\n100149\n25\n54\n1590\n18\nfalse\nNA\nvariable\nhomogeneous\n6\nabsent\n0.63758\n1\nvertical\nvertical\n1\n1.09411\nred\n2\nNaN\n1020.84\n727.5\nNaN\nNaN\n800.0\n855.0\n1020.84\n727.5\n1020.84\n472.5\n800.0\n345.0\n579.164\n472.5\n579.164\n727.5\n0\n4\n0\n0.0\n0.0\n0.0\nvertical\nvertical\nhorizontal\nhorizontal\nvertical\nhorizontal\n\n\n38391\n1c\n9\n100149\n25\n55\n1591\n18\nfalse\nNA\nvariable\nhomogeneous\n5\npresent\n0.604192\n1\nhorizontal\nhorizontal\n1\n1.08235\nred\n2\n3.0\n1046.31\n534.001\n813.346\n345.349\n938.883\n813.861\n1046.31\n534.001\n813.346\n345.349\n561.937\n508.616\n639.523\n798.172\n0.0\n0.0\n0\n4\n0\n0.0\n0.0\nNaN\nhorizontal\nhorizontal\nhorizontal\nhorizontal\nvertical\nnone\n\n\n38392\n1c\n9\n100149\n25\n56\n1592\n18\nfalse\nNA\nvariable\nhomogeneous\n3\nabsent\n0.670488\n1\nhorizontal\nhorizontal\n1\n0.905879\nred\n3\nNaN\n653.738\n391.116\nNaN\nNaN\n692.232\n831.108\n1054.03\n577.775\n653.738\n391.116\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0\n0\n4\nNaN\nNaN\nNaN\nhorizontal\nvertical\nhorizontal\nnone\nnone\nnone\n\n\n38393\n1c\n9\n100149\n25\n57\n1593\n18\nfalse\nNA\nvariable\nhomogeneous\n4\nabsent\n0.66062\n1\nhorizontal\nhorizontal\n1\n1.02352\ngreen\n1\nNaN\n643.006\n800.943\nNaN\nNaN\n643.006\n800.943\n1000.94\n756.994\n956.994\n399.057\n599.057\n443.006\n0.0\n0.0\n0.0\n0.0\n4\n0\n0\n0.0\nNaN\nNaN\nhorizontal\nhorizontal\nvertical\nvertical\nnone\nnone\n\n\n38394\n1c\n9\n100149\n25\n58\n1594\n18\nfalse\nNA\nvariable\nhomogeneous\n4\nabsent\n0.618374\n1\nhorizontal\nhorizontal\n1\n0.811762\ngreen\n2\nNaN\n1027.21\n484.232\nNaN\nNaN\n915.768\n827.207\n1027.21\n484.232\n684.232\n372.793\n572.793\n715.768\n0.0\n0.0\n0.0\n0.0\n0\n4\n0\n0.0\nNaN\nNaN\nvertical\nhorizontal\nhorizontal\nvertical\nnone\nnone\n\n\n38395\n1c\n9\n100149\n25\n59\n1595\n18\nfalse\nNA\nvariable\nhomogeneous\n5\nabsent\n0.573223\n1\nvertical\nvertical\n1\n0.717646\nred\n3\nNaN\n998.172\n439.523\nNaN\nNaN\n734.001\n846.311\n1013.86\n738.883\n998.172\n439.523\n708.616\n361.937\n545.349\n613.346\n0.0\n0.0\n0\n0\n4\n0.0\n0.0\nNaN\nvertical\nvertical\nvertical\nvertical\nhorizontal\nnone\n\n\n38396\n1c\n9\n100149\n25\n60\n1596\n18\nfalse\nNA\nvariable\nhomogeneous\n5\npresent\n0.54497\n1\nhorizontal\nhorizontal\n1\n0.811762\nred\n3\n2.0\n887.215\n360.378\n1054.84\n608.899\n870.288\n845.122\n1054.84\n608.899\n887.215\n360.378\n599.057\n443.006\n588.595\n742.594\n0.0\n0.0\n0\n0\n4\n0.0\n0.0\nNaN\nvertical\nhorizontal\nhorizontal\nhorizontal\nvertical\nnone\n\n\n38397\n1c\n9\n100149\n25\n61\n1597\n18\nfalse\nNA\nvariable\nhomogeneous\n4\nabsent\n0.552039\n1\nvertical\nvertical\n1\n0.799998\ngreen\n1\nNaN\n887.215\n839.622\nNaN\nNaN\n887.215\n839.622\n1039.62\n512.785\n712.785\n360.378\n560.378\n687.215\n0.0\n0.0\n0.0\n0.0\n4\n0\n0\n0.0\nNaN\nNaN\nvertical\nvertical\nhorizontal\nvertical\nnone\nnone\n\n\n38398\n1c\n9\n100149\n25\n62\n1598\n18\nfalse\nNA\nvariable\nhomogeneous\n3\nabsent\n0.551724\n1\nhorizontal\nhorizontal\n1\n0.635294\ngreen\n2\nNaN\n973.91\n413.505\nNaN\nNaN\n874.555\n843.858\n973.91\n413.505\n551.536\n542.637\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0\n4\n0\nNaN\nNaN\nNaN\nhorizontal\nhorizontal\nhorizontal\nnone\nnone\nnone\n\n\n38399\n1c\n9\n100149\n25\n63\n1599\n18\nfalse\nNA\nvariable\nhomogeneous\n5\nabsent\n0.575129\n1\nvertical\nvertical\n1\n0.788233\ngreen\n1\nNaN\n708.616\n838.063\nNaN\nNaN\n708.616\n838.063\n998.172\n760.477\n1013.86\n461.117\n734.001\n353.689\n545.349\n586.654\n0.0\n0.0\n4\n0\n0\n0.0\n0.0\nNaN\nvertical\nhorizontal\nvertical\nhorizontal\nvertical\nnone\n\n\n38400\n1c\n9\n100149\n25\n64\n1600\n18\nfalse\nNA\nvariable\nhomogeneous\n3\nabsent\n0.599166\n1\nhorizontal\nhorizontal\n1\n0.811762\nred\n2\nNaN\n1023.03\n476.374\nNaN\nNaN\n795.55\n854.961\n1023.03\n476.374\n581.422\n468.665\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0\n4\n0\nNaN\nNaN\nNaN\nvertical\nhorizontal\nvertical\nnone\nnone\nnone\n\n\n\n\n\n\nOnce it’s imported, we can take a look at our data, e.g., looking at subject means between the two conditions:\n\n@chain df begin\n  @subset(\n    :acc.==1, \n    :set_size.==4)\n  @transform(\n    :rt = :rt .* 1000,\n    :subject = string.(:subject)\n  )\n  groupby([:subject, :distractor])\n  @combine(\n    :rt = mean(:rt))\n  plot(\n    layer(\n    x=:distractor,\n    y=:rt,\n    color=:subject,\n    Geom.point\n    ),\n    layer(\n    x=:distractor,\n    y=:rt,\n    color=:subject,\n    Geom.line\n    ),\n    Theme(key_position=:none)\n  )\n  end\n\n\n\n\n\n\n  \n    \n  \n\n\n  \n    \n      \n        distractor\n      \n    \n  \n  \n    \n      \n        absent\n      \n    \n    \n      \n        present\n      \n    \n  \n  \n    \n      \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n        \n      \n      \n        \n          \n          \n        \n        \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n        \n        \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2664.3486083770285\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1634.315013885498\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1596.5403305987517\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2597.4775025719091\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1746.2136491301096\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2771.2027612997562\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2846.6547551907992\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1776.7736798241026\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2662.5892441682141\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1641.7468761911198\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2696.1439915889285\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1676.9537345115384\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2772.5133920453259\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1750.2685938126002\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1490.89349233187164\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2514.1337838089257\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2562.5185446536289\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1539.6714730465666\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1592.9642759050643\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2627.69085949004\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2663.0655116722234\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1601.2144048562211\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1671.9927034879986\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2694.1944373281377\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2634.0596158668478\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1616.8303174316568\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2886.5406752894164\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1850.874905733718\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2570.451065437081\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1554.8148142794768\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1589.2501879961063\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2612.9027163982391\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1611.0502781012119\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2628.3144865717206\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1708.5064026304912\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2737.689866920827\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2693.1869350584213\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1658.5283425933156\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2676.5534727985322\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1649.1643690692329\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2676.7373449948369\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1639.2413690836743\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2672.1931854072882\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1666.3701227030804\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2780.9169453879198\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1749.8016505661406\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2735.6533577567652\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1619.996827095747\n                \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n          \n            \n              \n                h,j,k,l,arrows,drag to pan\n              \n            \n            \n              \n                i,o,+,-,scroll,shift-drag to zoom\n              \n            \n            \n              \n                r,dbl-click to reset\n              \n            \n            \n              \n                c for coordinates\n              \n            \n            \n              \n                ? for help\n              \n            \n          \n        \n      \n      \n        \n          \n            \n              ?\n            \n          \n        \n      \n    \n  \n  \n    \n      \n        400\n      \n    \n    \n      \n        500\n      \n    \n    \n      \n        600\n      \n    \n    \n      \n        700\n      \n    \n    \n      \n        800\n      \n    \n    \n      \n        900\n      \n    \n    \n      \n        400\n      \n    \n    \n      \n        420\n      \n    \n    \n      \n        440\n      \n    \n    \n      \n        460\n      \n    \n    \n      \n        480\n      \n    \n    \n      \n        500\n      \n    \n    \n      \n        520\n      \n    \n    \n      \n        540\n      \n    \n    \n      \n        560\n      \n    \n    \n      \n        580\n      \n    \n    \n      \n        600\n      \n    \n    \n      \n        620\n      \n    \n    \n      \n        640\n      \n    \n    \n      \n        660\n      \n    \n    \n      \n        680\n      \n    \n    \n      \n        700\n      \n    \n    \n      \n        720\n      \n    \n    \n      \n        740\n      \n    \n    \n      \n        760\n      \n    \n    \n      \n        780\n      \n    \n    \n      \n        800\n      \n    \n    \n      \n        820\n      \n    \n    \n      \n        840\n      \n    \n    \n      \n        860\n      \n    \n    \n      \n        880\n      \n    \n    \n      \n        900\n      \n    \n    \n      \n        400\n      \n    \n    \n      \n        402\n      \n    \n    \n      \n        404\n      \n    \n    \n      \n        406\n      \n    \n    \n      \n        408\n      \n    \n    \n      \n        410\n      \n    \n    \n      \n        412\n      \n    \n    \n      \n        414\n      \n    \n    \n      \n        416\n      \n    \n    \n      \n        418\n      \n    \n    \n      \n        420\n      \n    \n    \n      \n        422\n      \n    \n    \n      \n        424\n      \n    \n    \n      \n        426\n      \n    \n    \n      \n        428\n      \n    \n    \n      \n        430\n      \n    \n    \n      \n        432\n      \n    \n    \n      \n        434\n      \n    \n    \n      \n        436\n      \n    \n    \n      \n        438\n      \n    \n    \n      \n        440\n      \n    \n    \n      \n        442\n      \n    \n    \n      \n        444\n      \n    \n    \n      \n        446\n      \n    \n    \n      \n        448\n      \n    \n    \n      \n        450\n      \n    \n    \n      \n        452\n      \n    \n    \n      \n        454\n      \n    \n    \n      \n        456\n      \n    \n    \n      \n        458\n      \n    \n    \n      \n        460\n      \n    \n    \n      \n        462\n      \n    \n    \n      \n        464\n      \n    \n    \n      \n        466\n      \n    \n    \n      \n        468\n      \n    \n    \n      \n        470\n      \n    \n    \n      \n        472\n      \n    \n    \n      \n        474\n      \n    \n    \n      \n        476\n      \n    \n    \n      \n        478\n      \n    \n    \n      \n        480\n      \n    \n    \n      \n        482\n      \n    \n    \n      \n        484\n      \n    \n    \n      \n        486\n      \n    \n    \n      \n        488\n      \n    \n    \n      \n        490\n      \n    \n    \n      \n        492\n      \n    \n    \n      \n        494\n      \n    \n    \n      \n        496\n      \n    \n    \n      \n        498\n      \n    \n    \n      \n        500\n      \n    \n    \n      \n        502\n      \n    \n    \n      \n        504\n      \n    \n    \n      \n        506\n      \n    \n    \n      \n        508\n      \n    \n    \n      \n        510\n      \n    \n    \n      \n        512\n      \n    \n    \n      \n        514\n      \n    \n    \n      \n        516\n      \n    \n    \n      \n        518\n      \n    \n    \n      \n        520\n      \n    \n    \n      \n        522\n      \n    \n    \n      \n        524\n      \n    \n    \n      \n        526\n      \n    \n    \n      \n        528\n      \n    \n    \n      \n        530\n      \n    \n    \n      \n        532\n      \n    \n    \n      \n        534\n      \n    \n    \n      \n        536\n      \n    \n    \n      \n        538\n      \n    \n    \n      \n        540\n      \n    \n    \n      \n        542\n      \n    \n    \n      \n        544\n      \n    \n    \n      \n        546\n      \n    \n    \n      \n        548\n      \n    \n    \n      \n        550\n      \n    \n    \n      \n        552\n      \n    \n    \n      \n        554\n      \n    \n    \n      \n        556\n      \n    \n    \n      \n        558\n      \n    \n    \n      \n        560\n      \n    \n    \n      \n        562\n      \n    \n    \n      \n        564\n      \n    \n    \n      \n        566\n      \n    \n    \n      \n        568\n      \n    \n    \n      \n        570\n      \n    \n    \n      \n        572\n      \n    \n    \n      \n        574\n      \n    \n    \n      \n        576\n      \n    \n    \n      \n        578\n      \n    \n    \n      \n        580\n      \n    \n    \n      \n        582\n      \n    \n    \n      \n        584\n      \n    \n    \n      \n        586\n      \n    \n    \n      \n        588\n      \n    \n    \n      \n        590\n      \n    \n    \n      \n        592\n      \n    \n    \n      \n        594\n      \n    \n    \n      \n        596\n      \n    \n    \n      \n        598\n      \n    \n    \n      \n        600\n      \n    \n    \n      \n        602\n      \n    \n    \n      \n        604\n      \n    \n    \n      \n        606\n      \n    \n    \n      \n        608\n      \n    \n    \n      \n        610\n      \n    \n    \n      \n        612\n      \n    \n    \n      \n        614\n      \n    \n    \n      \n        616\n      \n    \n    \n      \n        618\n      \n    \n    \n      \n        620\n      \n    \n    \n      \n        622\n      \n    \n    \n      \n        624\n      \n    \n    \n      \n        626\n      \n    \n    \n      \n        628\n      \n    \n    \n      \n        630\n      \n    \n    \n      \n        632\n      \n    \n    \n      \n        634\n      \n    \n    \n      \n        636\n      \n    \n    \n      \n        638\n      \n    \n    \n      \n        640\n      \n    \n    \n      \n        642\n      \n    \n    \n      \n        644\n      \n    \n    \n      \n        646\n      \n    \n    \n      \n        648\n      \n    \n    \n      \n        650\n      \n    \n    \n      \n        652\n      \n    \n    \n      \n        654\n      \n    \n    \n      \n        656\n      \n    \n    \n      \n        658\n      \n    \n    \n      \n        660\n      \n    \n    \n      \n        662\n      \n    \n    \n      \n        664\n      \n    \n    \n      \n        666\n      \n    \n    \n      \n        668\n      \n    \n    \n      \n        670\n      \n    \n    \n      \n        672\n      \n    \n    \n      \n        674\n      \n    \n    \n      \n        676\n      \n    \n    \n      \n        678\n      \n    \n    \n      \n        680\n      \n    \n    \n      \n        682\n      \n    \n    \n      \n        684\n      \n    \n    \n      \n        686\n      \n    \n    \n      \n        688\n      \n    \n    \n      \n        690\n      \n    \n    \n      \n        692\n      \n    \n    \n      \n        694\n      \n    \n    \n      \n        696\n      \n    \n    \n      \n        698\n      \n    \n    \n      \n        700\n      \n    \n    \n      \n        702\n      \n    \n    \n      \n        704\n      \n    \n    \n      \n        706\n      \n    \n    \n      \n        708\n      \n    \n    \n      \n        710\n      \n    \n    \n      \n        712\n      \n    \n    \n      \n        714\n      \n    \n    \n      \n        716\n      \n    \n    \n      \n        718\n      \n    \n    \n      \n        720\n      \n    \n    \n      \n        722\n      \n    \n    \n      \n        724\n      \n    \n    \n      \n        726\n      \n    \n    \n      \n        728\n      \n    \n    \n      \n        730\n      \n    \n    \n      \n        732\n      \n    \n    \n      \n        734\n      \n    \n    \n      \n        736\n      \n    \n    \n      \n        738\n      \n    \n    \n      \n        740\n      \n    \n    \n      \n        742\n      \n    \n    \n      \n        744\n      \n    \n    \n      \n        746\n      \n    \n    \n      \n        748\n      \n    \n    \n      \n        750\n      \n    \n    \n      \n        752\n      \n    \n    \n      \n        754\n      \n    \n    \n      \n        756\n      \n    \n    \n      \n        758\n      \n    \n    \n      \n        760\n      \n    \n    \n      \n        762\n      \n    \n    \n      \n        764\n      \n    \n    \n      \n        766\n      \n    \n    \n      \n        768\n      \n    \n    \n      \n        770\n      \n    \n    \n      \n        772\n      \n    \n    \n      \n        774\n      \n    \n    \n      \n        776\n      \n    \n    \n      \n        778\n      \n    \n    \n      \n        780\n      \n    \n    \n      \n        782\n      \n    \n    \n      \n        784\n      \n    \n    \n      \n        786\n      \n    \n    \n      \n        788\n      \n    \n    \n      \n        790\n      \n    \n    \n      \n        792\n      \n    \n    \n      \n        794\n      \n    \n    \n      \n        796\n      \n    \n    \n      \n        798\n      \n    \n    \n      \n        800\n      \n    \n    \n      \n        802\n      \n    \n    \n      \n        804\n      \n    \n    \n      \n        806\n      \n    \n    \n      \n        808\n      \n    \n    \n      \n        810\n      \n    \n    \n      \n        812\n      \n    \n    \n      \n        814\n      \n    \n    \n      \n        816\n      \n    \n    \n      \n        818\n      \n    \n    \n      \n        820\n      \n    \n    \n      \n        822\n      \n    \n    \n      \n        824\n      \n    \n    \n      \n        826\n      \n    \n    \n      \n        828\n      \n    \n    \n      \n        830\n      \n    \n    \n      \n        832\n      \n    \n    \n      \n        834\n      \n    \n    \n      \n        836\n      \n    \n    \n      \n        838\n      \n    \n    \n      \n        840\n      \n    \n    \n      \n        842\n      \n    \n    \n      \n        844\n      \n    \n    \n      \n        846\n      \n    \n    \n      \n        848\n      \n    \n    \n      \n        850\n      \n    \n    \n      \n        852\n      \n    \n    \n      \n        854\n      \n    \n    \n      \n        856\n      \n    \n    \n      \n        858\n      \n    \n    \n      \n        860\n      \n    \n    \n      \n        862\n      \n    \n    \n      \n        864\n      \n    \n    \n      \n        866\n      \n    \n    \n      \n        868\n      \n    \n    \n      \n        870\n      \n    \n    \n      \n        872\n      \n    \n    \n      \n        874\n      \n    \n    \n      \n        876\n      \n    \n    \n      \n        878\n      \n    \n    \n      \n        880\n      \n    \n    \n      \n        882\n      \n    \n    \n      \n        884\n      \n    \n    \n      \n        886\n      \n    \n    \n      \n        888\n      \n    \n    \n      \n        890\n      \n    \n    \n      \n        892\n      \n    \n    \n      \n        894\n      \n    \n    \n      \n        896\n      \n    \n    \n      \n        898\n      \n    \n    \n      \n        900\n      \n    \n    \n      \n        300\n      \n    \n    \n      \n        600\n      \n    \n    \n      \n        900\n      \n    \n  \n  \n    \n      \n        rt\n      \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\nWe can clearly see typical atttentional capture effects in the data. Now that we have the data, let’s model it:\n\nd = @chain df begin\n  @subset(\n    :acc.==1,\n    :set_size.==4)\n  @transform(\n    :rt = :rt.*1000,\n    :subject = string.(:subject))\nend\n\n# Our model is simple: RT is dependent on distractor presence, with a random slope and intercept for each subject. More complex models are left as an exercise to the reader.\n\nformula = @formula(rt ~ 1 + distractor + (1 + distractor | subject))\nformula_null = @formula(rt ~ 1 + (1 + distractor | subject))\n\nfm1 = fit(MixedModel, formula, d)\nfm1_null = fit(MixedModel, formula_null, d)\nlrt = MixedModels.likelihoodratiotest(fm1,fm1_null)\n\nfm1\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subject\n\n\n\n\n(Intercept)\n651.5037\n16.3119\n39.94\n&lt;1e-99\n78.8911\n\n\ndistractor: present\n30.8509\n4.6520\n6.63\n&lt;1e-10\n13.9796\n\n\nResidual\n175.8141\n\n\n\n\n\n\n\n\n\nThe above model rt ~ distractor + ( distractor | subject) is our putative data generating process, the parameters that we believe underly the generation of observed dependent variables, and the relationship between those parameters. The table shown above gives us parameter estimates for all fixed and random effects in the model. Now let’s plug those parameters into a simulation!\n\nn_subj     = 10     # number of subjects\nn_present  = 200    # number of distractor present trials\nn_absent   = 200    # number of distractor absent\nbeta_0     = 650    # Intercept\nbeta_1     = 30     # effect of category\ntau_0      = 80     # by-subject random intercept sd\ntau_1      = 15     # by-subject random slope sd\nrho        = 0.35   # correlation between intercept and slope\nsigma      = 175    # residual nose\n\n175\n\n\nGenerate trials with their fixed effects:\n\n# simulate a sample of items\n# total number of items = n_ingroup + n_outgroup\n\nitems = @chain DataFrame(\n  :item_id =&gt; range(1,n_absent+n_present),\n  :category =&gt; [repeat([\"absent\"],n_absent)..., repeat([\"present\"], n_present)...]\n) begin\n  @rtransform :X_i = :category == \"present\" ? 1 : 0\nend\n\n# items.describe()\n\n400×3 DataFrame375 rows omitted\n\n\n\nRow\nitem_id\ncategory\nX_i\n\n\n\nInt64\nString\nInt64\n\n\n\n\n1\n1\nabsent\n0\n\n\n2\n2\nabsent\n0\n\n\n3\n3\nabsent\n0\n\n\n4\n4\nabsent\n0\n\n\n5\n5\nabsent\n0\n\n\n6\n6\nabsent\n0\n\n\n7\n7\nabsent\n0\n\n\n8\n8\nabsent\n0\n\n\n9\n9\nabsent\n0\n\n\n10\n10\nabsent\n0\n\n\n11\n11\nabsent\n0\n\n\n12\n12\nabsent\n0\n\n\n13\n13\nabsent\n0\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n389\n389\npresent\n1\n\n\n390\n390\npresent\n1\n\n\n391\n391\npresent\n1\n\n\n392\n392\npresent\n1\n\n\n393\n393\npresent\n1\n\n\n394\n394\npresent\n1\n\n\n395\n395\npresent\n1\n\n\n396\n396\npresent\n1\n\n\n397\n397\npresent\n1\n\n\n398\n398\npresent\n1\n\n\n399\n399\npresent\n1\n\n\n400\n400\npresent\n1\n\n\n\n\n\n\nAnd generate participants with their random intercepts and slopes:\n\n#simulate a sample of subjects\n\n#calculate random intercept / random slope covariance\ncovar = rho * tau_0 * tau_1\n\n#put values into variance-covariance matrix\ncov_mx = [tau_0^2 covar; covar tau_1^2]\n\n#generate the by-subject random effects\ndist = MvNormal([0, 0], cov_mx)\nsubject_rfx = rand(dist, n_subj)\n\n# # combine with subject IDs\n\nsubjects = DataFrame(\n  :subj_id =&gt; string.(range(1,n_subj)),\n  :T_0s =&gt; subject_rfx[1,:],\n  :T_1s =&gt; subject_rfx[2,:]\n)\n\n10×3 DataFrame\n\n\n\nRow\nsubj_id\nT_0s\nT_1s\n\n\n\nString\nFloat64\nFloat64\n\n\n\n\n1\n1\n-67.9967\n-4.29486\n\n\n2\n2\n105.907\n-1.89717\n\n\n3\n3\n73.7466\n2.99368\n\n\n4\n4\n156.603\n14.0334\n\n\n5\n5\n40.9051\n-2.73222\n\n\n6\n6\n34.1562\n11.3813\n\n\n7\n7\n-74.5877\n-5.30155\n\n\n8\n8\n-80.6553\n21.5753\n\n\n9\n9\n126.907\n34.0272\n\n\n10\n10\n57.3057\n17.3225\n\n\n\n\n\n\nNow combine and add residual noise to create a complete dataframe:\n\n#cross items and subjects, add noise\n\ndat_sim = @chain crossjoin(subjects,items) begin\n  @rtransform @astable begin\n    :e_si = rand(Normal(0, sigma), 1)[1]\n    #calculate response variable\n    :RT = beta_0 + :T_0s + (beta_1 + :T_1s) * :X_i + :e_si\n    end\n  @select(:subj_id, :item_id, :category, :X_i, :RT)\nend\n\n4000×5 DataFrame3975 rows omitted\n\n\n\nRow\nsubj_id\nitem_id\ncategory\nX_i\nRT\n\n\n\nString\nInt64\nString\nInt64\nFloat64\n\n\n\n\n1\n1\n1\nabsent\n0\n516.535\n\n\n2\n1\n2\nabsent\n0\n757.313\n\n\n3\n1\n3\nabsent\n0\n544.622\n\n\n4\n1\n4\nabsent\n0\n655.981\n\n\n5\n1\n5\nabsent\n0\n691.763\n\n\n6\n1\n6\nabsent\n0\n523.838\n\n\n7\n1\n7\nabsent\n0\n510.876\n\n\n8\n1\n8\nabsent\n0\n690.3\n\n\n9\n1\n9\nabsent\n0\n792.548\n\n\n10\n1\n10\nabsent\n0\n713.432\n\n\n11\n1\n11\nabsent\n0\n825.375\n\n\n12\n1\n12\nabsent\n0\n627.622\n\n\n13\n1\n13\nabsent\n0\n667.263\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n3989\n10\n389\npresent\n1\n531.323\n\n\n3990\n10\n390\npresent\n1\n800.412\n\n\n3991\n10\n391\npresent\n1\n527.266\n\n\n3992\n10\n392\npresent\n1\n787.222\n\n\n3993\n10\n393\npresent\n1\n664.524\n\n\n3994\n10\n394\npresent\n1\n701.211\n\n\n3995\n10\n395\npresent\n1\n562.528\n\n\n3996\n10\n396\npresent\n1\n677.546\n\n\n3997\n10\n397\npresent\n1\n527.31\n\n\n3998\n10\n398\npresent\n1\n686.81\n\n\n3999\n10\n399\npresent\n1\n736.427\n\n\n4000\n10\n400\npresent\n1\n667.098\n\n\n\n\n\n\nData generated! Does it look like we’d expect?\n\n@chain dat_sim begin\n  groupby([:subj_id, :category])\n  @combine(\n    :RT = mean(:RT))\n  plot(\n    layer(\n    x=:category,\n    y=:RT,\n    color=:subj_id,\n    Geom.point\n    ),\n    layer(\n    x=:category,\n    y=:RT,\n    color=:subj_id,\n    Geom.line\n    ),\n    Theme(key_position=:none)\n  )\n  end\n\n\n\n\n\n\n  \n    \n  \n\n\n  \n    \n      \n        category\n      \n    \n  \n  \n    \n      \n        absent\n      \n    \n    \n      \n        present\n      \n    \n  \n  \n    \n      \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n        \n      \n      \n        \n          \n          \n        \n        \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n          \n            \n          \n        \n        \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2747.7810949908085\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1698.5580646874738\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2843.3112150118571\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1770.4761976702373\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2607.4369942269565\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1577.7904606931501\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2602.7471673021282\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1582.7455560915758\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2728.6172898775102\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1680.1664765350131\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2707.034138874286\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1680.856472088064\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2857.060791938592\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1795.6516553027382\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2759.42905154613\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1763.5041656052523\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2792.2599495859052\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1744.266231182607\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2626.1312264583478\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1602.2390899766315\n                \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n          \n            \n              \n                h,j,k,l,arrows,drag to pan\n              \n            \n            \n              \n                i,o,+,-,scroll,shift-drag to zoom\n              \n            \n            \n              \n                r,dbl-click to reset\n              \n            \n            \n              \n                c for coordinates\n              \n            \n            \n              \n                ? for help\n              \n            \n          \n        \n      \n      \n        \n          \n            \n              ?\n            \n          \n        \n      \n    \n  \n  \n    \n      \n        500\n      \n    \n    \n      \n        600\n      \n    \n    \n      \n        700\n      \n    \n    \n      \n        800\n      \n    \n    \n      \n        900\n      \n    \n    \n      \n        500\n      \n    \n    \n      \n        520\n      \n    \n    \n      \n        540\n      \n    \n    \n      \n        560\n      \n    \n    \n      \n        580\n      \n    \n    \n      \n        600\n      \n    \n    \n      \n        620\n      \n    \n    \n      \n        640\n      \n    \n    \n      \n        660\n      \n    \n    \n      \n        680\n      \n    \n    \n      \n        700\n      \n    \n    \n      \n        720\n      \n    \n    \n      \n        740\n      \n    \n    \n      \n        760\n      \n    \n    \n      \n        780\n      \n    \n    \n      \n        800\n      \n    \n    \n      \n        820\n      \n    \n    \n      \n        840\n      \n    \n    \n      \n        860\n      \n    \n    \n      \n        880\n      \n    \n    \n      \n        900\n      \n    \n    \n      \n        500\n      \n    \n    \n      \n        502\n      \n    \n    \n      \n        504\n      \n    \n    \n      \n        506\n      \n    \n    \n      \n        508\n      \n    \n    \n      \n        510\n      \n    \n    \n      \n        512\n      \n    \n    \n      \n        514\n      \n    \n    \n      \n        516\n      \n    \n    \n      \n        518\n      \n    \n    \n      \n        520\n      \n    \n    \n      \n        522\n      \n    \n    \n      \n        524\n      \n    \n    \n      \n        526\n      \n    \n    \n      \n        528\n      \n    \n    \n      \n        530\n      \n    \n    \n      \n        532\n      \n    \n    \n      \n        534\n      \n    \n    \n      \n        536\n      \n    \n    \n      \n        538\n      \n    \n    \n      \n        540\n      \n    \n    \n      \n        542\n      \n    \n    \n      \n        544\n      \n    \n    \n      \n        546\n      \n    \n    \n      \n        548\n      \n    \n    \n      \n        550\n      \n    \n    \n      \n        552\n      \n    \n    \n      \n        554\n      \n    \n    \n      \n        556\n      \n    \n    \n      \n        558\n      \n    \n    \n      \n        560\n      \n    \n    \n      \n        562\n      \n    \n    \n      \n        564\n      \n    \n    \n      \n        566\n      \n    \n    \n      \n        568\n      \n    \n    \n      \n        570\n      \n    \n    \n      \n        572\n      \n    \n    \n      \n        574\n      \n    \n    \n      \n        576\n      \n    \n    \n      \n        578\n      \n    \n    \n      \n        580\n      \n    \n    \n      \n        582\n      \n    \n    \n      \n        584\n      \n    \n    \n      \n        586\n      \n    \n    \n      \n        588\n      \n    \n    \n      \n        590\n      \n    \n    \n      \n        592\n      \n    \n    \n      \n        594\n      \n    \n    \n      \n        596\n      \n    \n    \n      \n        598\n      \n    \n    \n      \n        600\n      \n    \n    \n      \n        602\n      \n    \n    \n      \n        604\n      \n    \n    \n      \n        606\n      \n    \n    \n      \n        608\n      \n    \n    \n      \n        610\n      \n    \n    \n      \n        612\n      \n    \n    \n      \n        614\n      \n    \n    \n      \n        616\n      \n    \n    \n      \n        618\n      \n    \n    \n      \n        620\n      \n    \n    \n      \n        622\n      \n    \n    \n      \n        624\n      \n    \n    \n      \n        626\n      \n    \n    \n      \n        628\n      \n    \n    \n      \n        630\n      \n    \n    \n      \n        632\n      \n    \n    \n      \n        634\n      \n    \n    \n      \n        636\n      \n    \n    \n      \n        638\n      \n    \n    \n      \n        640\n      \n    \n    \n      \n        642\n      \n    \n    \n      \n        644\n      \n    \n    \n      \n        646\n      \n    \n    \n      \n        648\n      \n    \n    \n      \n        650\n      \n    \n    \n      \n        652\n      \n    \n    \n      \n        654\n      \n    \n    \n      \n        656\n      \n    \n    \n      \n        658\n      \n    \n    \n      \n        660\n      \n    \n    \n      \n        662\n      \n    \n    \n      \n        664\n      \n    \n    \n      \n        666\n      \n    \n    \n      \n        668\n      \n    \n    \n      \n        670\n      \n    \n    \n      \n        672\n      \n    \n    \n      \n        674\n      \n    \n    \n      \n        676\n      \n    \n    \n      \n        678\n      \n    \n    \n      \n        680\n      \n    \n    \n      \n        682\n      \n    \n    \n      \n        684\n      \n    \n    \n      \n        686\n      \n    \n    \n      \n        688\n      \n    \n    \n      \n        690\n      \n    \n    \n      \n        692\n      \n    \n    \n      \n        694\n      \n    \n    \n      \n        696\n      \n    \n    \n      \n        698\n      \n    \n    \n      \n        700\n      \n    \n    \n      \n        702\n      \n    \n    \n      \n        704\n      \n    \n    \n      \n        706\n      \n    \n    \n      \n        708\n      \n    \n    \n      \n        710\n      \n    \n    \n      \n        712\n      \n    \n    \n      \n        714\n      \n    \n    \n      \n        716\n      \n    \n    \n      \n        718\n      \n    \n    \n      \n        720\n      \n    \n    \n      \n        722\n      \n    \n    \n      \n        724\n      \n    \n    \n      \n        726\n      \n    \n    \n      \n        728\n      \n    \n    \n      \n        730\n      \n    \n    \n      \n        732\n      \n    \n    \n      \n        734\n      \n    \n    \n      \n        736\n      \n    \n    \n      \n        738\n      \n    \n    \n      \n        740\n      \n    \n    \n      \n        742\n      \n    \n    \n      \n        744\n      \n    \n    \n      \n        746\n      \n    \n    \n      \n        748\n      \n    \n    \n      \n        750\n      \n    \n    \n      \n        752\n      \n    \n    \n      \n        754\n      \n    \n    \n      \n        756\n      \n    \n    \n      \n        758\n      \n    \n    \n      \n        760\n      \n    \n    \n      \n        762\n      \n    \n    \n      \n        764\n      \n    \n    \n      \n        766\n      \n    \n    \n      \n        768\n      \n    \n    \n      \n        770\n      \n    \n    \n      \n        772\n      \n    \n    \n      \n        774\n      \n    \n    \n      \n        776\n      \n    \n    \n      \n        778\n      \n    \n    \n      \n        780\n      \n    \n    \n      \n        782\n      \n    \n    \n      \n        784\n      \n    \n    \n      \n        786\n      \n    \n    \n      \n        788\n      \n    \n    \n      \n        790\n      \n    \n    \n      \n        792\n      \n    \n    \n      \n        794\n      \n    \n    \n      \n        796\n      \n    \n    \n      \n        798\n      \n    \n    \n      \n        800\n      \n    \n    \n      \n        802\n      \n    \n    \n      \n        804\n      \n    \n    \n      \n        806\n      \n    \n    \n      \n        808\n      \n    \n    \n      \n        810\n      \n    \n    \n      \n        812\n      \n    \n    \n      \n        814\n      \n    \n    \n      \n        816\n      \n    \n    \n      \n        818\n      \n    \n    \n      \n        820\n      \n    \n    \n      \n        822\n      \n    \n    \n      \n        824\n      \n    \n    \n      \n        826\n      \n    \n    \n      \n        828\n      \n    \n    \n      \n        830\n      \n    \n    \n      \n        832\n      \n    \n    \n      \n        834\n      \n    \n    \n      \n        836\n      \n    \n    \n      \n        838\n      \n    \n    \n      \n        840\n      \n    \n    \n      \n        842\n      \n    \n    \n      \n        844\n      \n    \n    \n      \n        846\n      \n    \n    \n      \n        848\n      \n    \n    \n      \n        850\n      \n    \n    \n      \n        852\n      \n    \n    \n      \n        854\n      \n    \n    \n      \n        856\n      \n    \n    \n      \n        858\n      \n    \n    \n      \n        860\n      \n    \n    \n      \n        862\n      \n    \n    \n      \n        864\n      \n    \n    \n      \n        866\n      \n    \n    \n      \n        868\n      \n    \n    \n      \n        870\n      \n    \n    \n      \n        872\n      \n    \n    \n      \n        874\n      \n    \n    \n      \n        876\n      \n    \n    \n      \n        878\n      \n    \n    \n      \n        880\n      \n    \n    \n      \n        882\n      \n    \n    \n      \n        884\n      \n    \n    \n      \n        886\n      \n    \n    \n      \n        888\n      \n    \n    \n      \n        890\n      \n    \n    \n      \n        892\n      \n    \n    \n      \n        894\n      \n    \n    \n      \n        896\n      \n    \n    \n      \n        898\n      \n    \n    \n      \n        900\n      \n    \n    \n      \n        500\n      \n    \n    \n      \n        1000\n      \n    \n  \n  \n    \n      \n        RT\n      \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\nLooks comparable to the original data! Now let’s fit a model to see if we recover the parameters:\n\nsimformula = @formula(RT ~ 1 + category + (1 + category | subj_id))\nsimformula_null = @formula(RT ~ 1 + (category | subj_id))\nmf2 = fit(MixedModel, simformula, dat_sim)\nmf2_null = fit(MixedModel, simformula_null, dat_sim)\nlrt_sim = MixedModels.likelihoodratiotest(mf2,mf2_null)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nRT ~ 1 + (1 + category | subj_id)\n5\n52724\n\n\n\n\n\nRT ~ 1 + category + (1 + category | subj_id)\n6\n52710\n14\n1\n0.0002\n\n\n\n\n\nGreat, our simulation works - we recover our ground truth for the different coefficients (allowing for differences due to noise and limited sample size). Now for a power analysis, we’d put the above in functions and run the code many times for a given combination of parameters. See below:\n\nfunction my_sim_data(;\n  n_subj     = 5,       # number of subjects\n  n_present  = 200,     # number of distractor present trials\n  n_absent   = 200,     # number of distractor absent\n  beta_0     = 650,     # Intercept\n  beta_1     = 30,      # effect of category\n  tau_0      = 80,      # by-subject random intercept sd\n  tau_1      = 15,      # by-subject random slope sd\n  rho        = 0.35,    # correlation between intercept and slope\n  sigma      = 175      # residual noise\n)\n  \n  # simulate a sample of items\n  # total number of items = n_ingroup + n_outgroup\n\n  items = @chain DataFrame(\n    :item_id =&gt; range(1,n_absent+n_present),\n    :category =&gt; [repeat([\"absent\"],n_absent)..., repeat([\"present\"], n_present)...]\n  ) begin\n    @rtransform :X_i = :category == \"present\" ? 1 : 0\n  end\n\n  #simulate a sample of subjects\n\n  #calculate random intercept / random slope covariance\n  covar = rho * tau_0 * tau_1\n\n  #put values into variance-covariance matrix\n  cov_mx = [tau_0^2 covar; covar tau_1^2]\n\n  #generate the by-subject random effects\n  dist = MvNormal([0, 0], cov_mx)\n  subject_rfx = rand(dist, n_subj)\n\n  # # combine with subject IDs\n\n  subjects = DataFrame(\n    :subj_id =&gt; string.(range(1,n_subj)),\n    :T_0s =&gt; subject_rfx[1,:],\n    :T_1s =&gt; subject_rfx[2,:]\n  )\n\n  dat_sim = @chain crossjoin(subjects,items) begin\n    @rtransform @astable begin\n      :e_si = rand(Normal(0, sigma), 1)[1]\n      #calculate response variable\n      :RT = beta_0 + :T_0s + (beta_1 + :T_1s) * :X_i + :e_si\n      end\n    @select(:subj_id, :item_id, :category, :X_i, :RT)\n  end\n  \n  dat_sim\nend\n\nmy_sim_data (generic function with 1 method)\n\n\nThe above function simulates data. The function below combines it with a model fit so we have a function that can be repeatedly called during our power analysis.\n\nfunction single_run(filename = nothing, args...; kwargs...)\n  \n  dat_sim = my_sim_data(;kwargs...)\n  simformula = @formula(RT ~ 1 + category + (1 + category | subj_id))\n  simformula_null = @formula(RT ~ 1 + (category | subj_id))\n  mf = fit(MixedModel, simformula, dat_sim) \n  mf_null = fit(MixedModel, simformula_null, dat_sim)\n  lrt = MixedModels.likelihoodratiotest(mf,mf_null)\n  sim_results = DataFrame(coeftable(mf))\n  # the p-values calculated by MixedModels are uncorrected and therefore inflated - we use the best (reasonable) alternative method available to us, the likelihood ratio test vs the intercept-only model, to find a better estimate (note that we leave the intercept p value since it isn't of interest here)\n  sim_results[!,\"Pr(&gt;|z|)\"][2] = lrt.pvalues[1]\n  if length([kwargs...]) != 0\n    sim_results = crossjoin(sim_results,DataFrame(kwargs...))\n  end\n  \n  if !isnothing(filename)\n    append = isfile(filename)\n    CSV.write(filename, sim_results, append = append)\n  end  \n  sim_results\nend\n\nsingle_run (generic function with 2 methods)\n\n\nNow let’s run our sensitivity analysis - we will run our simulation many times (1000+ times, which we can show here because of Julia’s blazing speed) for each combination of parameters, and record how often the fixed effect estimates reach significance:\n\nnreps = 1000\n\nparams = allcombinations(\n  DataFrame,\n  :rep        =&gt; 1:nreps, # number of runs\n  :n_subj     =&gt; 8,       # number of subjects\n  :n_present  =&gt; 150,     # number of distractor present trials\n  :n_absent   =&gt; 150,     # number of distractor absent\n  :beta_0     =&gt; 650,     # Intercept\n  :beta_1     =&gt; 30,      # effect of category\n  :tau_0      =&gt; 80,      # by-subject random intercept sd\n  :tau_1      =&gt; 15,      # by-subject random slope sd\n  :rho        =&gt; 0.35,    # correlation between intercept and slope\n  :sigma      =&gt; 175      # residual (standard deviation)\n)\n\nselect!(params,Not(\"rep\"))\n\nalpha = .05\n\nsims = vcat([single_run(;r...) for r in Tables.rowtable(params)]...)\n\n@chain sims begin\n  @select(:Name, $\"Pr(&gt;|z|)\")\n  @transform :p = $\"Pr(&gt;|z|)\" .&lt; alpha\n  groupby(:Name)\n  @combine :power = mean(:p)\nend\n\n2×2 DataFrame\n\n\n\nRow\nName\npower\n\n\n\nString\nFloat64\n\n\n\n\n1\n(Intercept)\n1.0\n\n\n2\ncategory: present\n0.874\n\n\n\n\n\n\nIf we want to run our sensitivity analysis across a given parameter space, we’ll have to map the function single_run to generate data across this space, for example, over a varying number of participants:\n\nfilename1 = \"sens_jl.csv\"\n\nnreps = 1000\n\nparams = allcombinations(\n  DataFrame,\n  :rep        =&gt; 1:nreps, # number of runs\n  :n_subj     =&gt; 2:15,    # number of subjects\n  :n_present  =&gt; 150,     # number of distractor present trials\n  :n_absent   =&gt; 150,     # number of distractor absent\n  :beta_0     =&gt; 650,     # Intercept\n  :beta_1     =&gt; 30,      # effect of category\n  :tau_0      =&gt; 80,      # by-subject random intercept sd\n  :tau_1      =&gt; 15,      # by-subject random slope sd\n  :rho        =&gt; 0.35,    # correlation between intercept and slope\n  :sigma      =&gt; 175      # residual (standard deviation)\n)\n\nselect!(params,Not(\"rep\"))\n\nalpha = 0.05\n\nif !isfile(filename1)\n  sims = vcat([single_run(filename1; r...) for r in Tables.rowtable(params)]...)\nend\n\nNote that the above could obviously also be run over other dimensions of our parameter space, e.g. for different estimates of the fixed effects, amount of noise, number of trials, etc. etc., by changing the params list. How did we do? Let’s take a look at our power curve.\n\nsims1 = CSV.read(\"sens_jl.csv\", DataFrame)\n\npower1 = @chain sims1 begin\n  @subset :Name .== \"category: present\"\n  @transform :p = $\"Pr(&gt;|z|)\" .&lt; alpha\n  groupby([:Name, :n_subj])\n  @combine begin\n  :mean_estimate = mean($\"Coef.\")\n  :mean_se = mean($\"Std. Error\")\n  :power = mean(:p)  \n  end\nend\n\n14×5 DataFrame\n\n\n\nRow\nName\nn_subj\nmean_estimate\nmean_se\npower\n\n\n\nString31\nInt64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\ncategory: present\n2\n30.2635\n17.9024\n0.205\n\n\n2\ncategory: present\n3\n29.526\n14.6008\n0.356\n\n\n3\ncategory: present\n4\n29.9583\n12.6264\n0.5\n\n\n4\ncategory: present\n5\n30.1647\n11.1628\n0.647\n\n\n5\ncategory: present\n6\n29.9987\n10.2055\n0.743\n\n\n6\ncategory: present\n7\n30.1957\n9.33679\n0.823\n\n\n7\ncategory: present\n8\n29.9215\n8.80461\n0.874\n\n\n8\ncategory: present\n9\n30.7667\n8.2574\n0.924\n\n\n9\ncategory: present\n10\n30.071\n7.75013\n0.956\n\n\n10\ncategory: present\n11\n29.4491\n7.43248\n0.945\n\n\n11\ncategory: present\n12\n30.256\n7.10302\n0.971\n\n\n12\ncategory: present\n13\n29.7838\n6.819\n0.983\n\n\n13\ncategory: present\n14\n29.8746\n6.62874\n0.983\n\n\n14\ncategory: present\n15\n29.8489\n6.42804\n0.997\n\n\n\n\n\n\n\nplot(\n  power1,\n  layer(\n    x=:n_subj,\n    y=:power,\n    Geom.smooth,\n    yintercept=[0.8],\n    Geom.hline\n  ),\n  layer(\n    x=:n_subj,\n    y=:power,\n    Geom.point\n  )\n)\n\n\n\n\n\n\n  \n    \n  \n\n\n  \n    \n      \n        n_subj\n      \n    \n  \n  \n    \n      \n        0\n      \n    \n    \n      \n        5\n      \n    \n    \n      \n        10\n      \n    \n    \n      \n        15\n      \n    \n    \n      \n        0.0\n      \n    \n    \n      \n        0.5\n      \n    \n    \n      \n        1.0\n      \n    \n    \n      \n        1.5\n      \n    \n    \n      \n        2.0\n      \n    \n    \n      \n        2.5\n      \n    \n    \n      \n        3.0\n      \n    \n    \n      \n        3.5\n      \n    \n    \n      \n        4.0\n      \n    \n    \n      \n        4.5\n      \n    \n    \n      \n        5.0\n      \n    \n    \n      \n        5.5\n      \n    \n    \n      \n        6.0\n      \n    \n    \n      \n        6.5\n      \n    \n    \n      \n        7.0\n      \n    \n    \n      \n        7.5\n      \n    \n    \n      \n        8.0\n      \n    \n    \n      \n        8.5\n      \n    \n    \n      \n        9.0\n      \n    \n    \n      \n        9.5\n      \n    \n    \n      \n        10.0\n      \n    \n    \n      \n        10.5\n      \n    \n    \n      \n        11.0\n      \n    \n    \n      \n        11.5\n      \n    \n    \n      \n        12.0\n      \n    \n    \n      \n        12.5\n      \n    \n    \n      \n        13.0\n      \n    \n    \n      \n        13.5\n      \n    \n    \n      \n        14.0\n      \n    \n    \n      \n        14.5\n      \n    \n    \n      \n        15.0\n      \n    \n    \n      \n        0.00\n      \n    \n    \n      \n        0.05\n      \n    \n    \n      \n        0.10\n      \n    \n    \n      \n        0.15\n      \n    \n    \n      \n        0.20\n      \n    \n    \n      \n        0.25\n      \n    \n    \n      \n        0.30\n      \n    \n    \n      \n        0.35\n      \n    \n    \n      \n        0.40\n      \n    \n    \n      \n        0.45\n      \n    \n    \n      \n        0.50\n      \n    \n    \n      \n        0.55\n      \n    \n    \n      \n        0.60\n      \n    \n    \n      \n        0.65\n      \n    \n    \n      \n        0.70\n      \n    \n    \n      \n        0.75\n      \n    \n    \n      \n        0.80\n      \n    \n    \n      \n        0.85\n      \n    \n    \n      \n        0.90\n      \n    \n    \n      \n        0.95\n      \n    \n    \n      \n        1.00\n      \n    \n    \n      \n        1.05\n      \n    \n    \n      \n        1.10\n      \n    \n    \n      \n        1.15\n      \n    \n    \n      \n        1.20\n      \n    \n    \n      \n        1.25\n      \n    \n    \n      \n        1.30\n      \n    \n    \n      \n        1.35\n      \n    \n    \n      \n        1.40\n      \n    \n    \n      \n        1.45\n      \n    \n    \n      \n        1.50\n      \n    \n    \n      \n        1.55\n      \n    \n    \n      \n        1.60\n      \n    \n    \n      \n        1.65\n      \n    \n    \n      \n        1.70\n      \n    \n    \n      \n        1.75\n      \n    \n    \n      \n        1.80\n      \n    \n    \n      \n        1.85\n      \n    \n    \n      \n        1.90\n      \n    \n    \n      \n        1.95\n      \n    \n    \n      \n        2.00\n      \n    \n    \n      \n        2.05\n      \n    \n    \n      \n        2.10\n      \n    \n    \n      \n        2.15\n      \n    \n    \n      \n        2.20\n      \n    \n    \n      \n        2.25\n      \n    \n    \n      \n        2.30\n      \n    \n    \n      \n        2.35\n      \n    \n    \n      \n        2.40\n      \n    \n    \n      \n        2.45\n      \n    \n    \n      \n        2.50\n      \n    \n    \n      \n        2.55\n      \n    \n    \n      \n        2.60\n      \n    \n    \n      \n        2.65\n      \n    \n    \n      \n        2.70\n      \n    \n    \n      \n        2.75\n      \n    \n    \n      \n        2.80\n      \n    \n    \n      \n        2.85\n      \n    \n    \n      \n        2.90\n      \n    \n    \n      \n        2.95\n      \n    \n    \n      \n        3.00\n      \n    \n    \n      \n        3.05\n      \n    \n    \n      \n        3.10\n      \n    \n    \n      \n        3.15\n      \n    \n    \n      \n        3.20\n      \n    \n    \n      \n        3.25\n      \n    \n    \n      \n        3.30\n      \n    \n    \n      \n        3.35\n      \n    \n    \n      \n        3.40\n      \n    \n    \n      \n        3.45\n      \n    \n    \n      \n        3.50\n      \n    \n    \n      \n        3.55\n      \n    \n    \n      \n        3.60\n      \n    \n    \n      \n        3.65\n      \n    \n    \n      \n        3.70\n      \n    \n    \n      \n        3.75\n      \n    \n    \n      \n        3.80\n      \n    \n    \n      \n        3.85\n      \n    \n    \n      \n        3.90\n      \n    \n    \n      \n        3.95\n      \n    \n    \n      \n        4.00\n      \n    \n    \n      \n        4.05\n      \n    \n    \n      \n        4.10\n      \n    \n    \n      \n        4.15\n      \n    \n    \n      \n        4.20\n      \n    \n    \n      \n        4.25\n      \n    \n    \n      \n        4.30\n      \n    \n    \n      \n        4.35\n      \n    \n    \n      \n        4.40\n      \n    \n    \n      \n        4.45\n      \n    \n    \n      \n        4.50\n      \n    \n    \n      \n        4.55\n      \n    \n    \n      \n        4.60\n      \n    \n    \n      \n        4.65\n      \n    \n    \n      \n        4.70\n      \n    \n    \n      \n        4.75\n      \n    \n    \n      \n        4.80\n      \n    \n    \n      \n        4.85\n      \n    \n    \n      \n        4.90\n      \n    \n    \n      \n        4.95\n      \n    \n    \n      \n        5.00\n      \n    \n    \n      \n        5.05\n      \n    \n    \n      \n        5.10\n      \n    \n    \n      \n        5.15\n      \n    \n    \n      \n        5.20\n      \n    \n    \n      \n        5.25\n      \n    \n    \n      \n        5.30\n      \n    \n    \n      \n        5.35\n      \n    \n    \n      \n        5.40\n      \n    \n    \n      \n        5.45\n      \n    \n    \n      \n        5.50\n      \n    \n    \n      \n        5.55\n      \n    \n    \n      \n        5.60\n      \n    \n    \n      \n        5.65\n      \n    \n    \n      \n        5.70\n      \n    \n    \n      \n        5.75\n      \n    \n    \n      \n        5.80\n      \n    \n    \n      \n        5.85\n      \n    \n    \n      \n        5.90\n      \n    \n    \n      \n        5.95\n      \n    \n    \n      \n        6.00\n      \n    \n    \n      \n        6.05\n      \n    \n    \n      \n        6.10\n      \n    \n    \n      \n        6.15\n      \n    \n    \n      \n        6.20\n      \n    \n    \n      \n        6.25\n      \n    \n    \n      \n        6.30\n      \n    \n    \n      \n        6.35\n      \n    \n    \n      \n        6.40\n      \n    \n    \n      \n        6.45\n      \n    \n    \n      \n        6.50\n      \n    \n    \n      \n        6.55\n      \n    \n    \n      \n        6.60\n      \n    \n    \n      \n        6.65\n      \n    \n    \n      \n        6.70\n      \n    \n    \n      \n        6.75\n      \n    \n    \n      \n        6.80\n      \n    \n    \n      \n        6.85\n      \n    \n    \n      \n        6.90\n      \n    \n    \n      \n        6.95\n      \n    \n    \n      \n        7.00\n      \n    \n    \n      \n        7.05\n      \n    \n    \n      \n        7.10\n      \n    \n    \n      \n        7.15\n      \n    \n    \n      \n        7.20\n      \n    \n    \n      \n        7.25\n      \n    \n    \n      \n        7.30\n      \n    \n    \n      \n        7.35\n      \n    \n    \n      \n        7.40\n      \n    \n    \n      \n        7.45\n      \n    \n    \n      \n        7.50\n      \n    \n    \n      \n        7.55\n      \n    \n    \n      \n        7.60\n      \n    \n    \n      \n        7.65\n      \n    \n    \n      \n        7.70\n      \n    \n    \n      \n        7.75\n      \n    \n    \n      \n        7.80\n      \n    \n    \n      \n        7.85\n      \n    \n    \n      \n        7.90\n      \n    \n    \n      \n        7.95\n      \n    \n    \n      \n        8.00\n      \n    \n    \n      \n        8.05\n      \n    \n    \n      \n        8.10\n      \n    \n    \n      \n        8.15\n      \n    \n    \n      \n        8.20\n      \n    \n    \n      \n        8.25\n      \n    \n    \n      \n        8.30\n      \n    \n    \n      \n        8.35\n      \n    \n    \n      \n        8.40\n      \n    \n    \n      \n        8.45\n      \n    \n    \n      \n        8.50\n      \n    \n    \n      \n        8.55\n      \n    \n    \n      \n        8.60\n      \n    \n    \n      \n        8.65\n      \n    \n    \n      \n        8.70\n      \n    \n    \n      \n        8.75\n      \n    \n    \n      \n        8.80\n      \n    \n    \n      \n        8.85\n      \n    \n    \n      \n        8.90\n      \n    \n    \n      \n        8.95\n      \n    \n    \n      \n        9.00\n      \n    \n    \n      \n        9.05\n      \n    \n    \n      \n        9.10\n      \n    \n    \n      \n        9.15\n      \n    \n    \n      \n        9.20\n      \n    \n    \n      \n        9.25\n      \n    \n    \n      \n        9.30\n      \n    \n    \n      \n        9.35\n      \n    \n    \n      \n        9.40\n      \n    \n    \n      \n        9.45\n      \n    \n    \n      \n        9.50\n      \n    \n    \n      \n        9.55\n      \n    \n    \n      \n        9.60\n      \n    \n    \n      \n        9.65\n      \n    \n    \n      \n        9.70\n      \n    \n    \n      \n        9.75\n      \n    \n    \n      \n        9.80\n      \n    \n    \n      \n        9.85\n      \n    \n    \n      \n        9.90\n      \n    \n    \n      \n        9.95\n      \n    \n    \n      \n        10.00\n      \n    \n    \n      \n        10.05\n      \n    \n    \n      \n        10.10\n      \n    \n    \n      \n        10.15\n      \n    \n    \n      \n        10.20\n      \n    \n    \n      \n        10.25\n      \n    \n    \n      \n        10.30\n      \n    \n    \n      \n        10.35\n      \n    \n    \n      \n        10.40\n      \n    \n    \n      \n        10.45\n      \n    \n    \n      \n        10.50\n      \n    \n    \n      \n        10.55\n      \n    \n    \n      \n        10.60\n      \n    \n    \n      \n        10.65\n      \n    \n    \n      \n        10.70\n      \n    \n    \n      \n        10.75\n      \n    \n    \n      \n        10.80\n      \n    \n    \n      \n        10.85\n      \n    \n    \n      \n        10.90\n      \n    \n    \n      \n        10.95\n      \n    \n    \n      \n        11.00\n      \n    \n    \n      \n        11.05\n      \n    \n    \n      \n        11.10\n      \n    \n    \n      \n        11.15\n      \n    \n    \n      \n        11.20\n      \n    \n    \n      \n        11.25\n      \n    \n    \n      \n        11.30\n      \n    \n    \n      \n        11.35\n      \n    \n    \n      \n        11.40\n      \n    \n    \n      \n        11.45\n      \n    \n    \n      \n        11.50\n      \n    \n    \n      \n        11.55\n      \n    \n    \n      \n        11.60\n      \n    \n    \n      \n        11.65\n      \n    \n    \n      \n        11.70\n      \n    \n    \n      \n        11.75\n      \n    \n    \n      \n        11.80\n      \n    \n    \n      \n        11.85\n      \n    \n    \n      \n        11.90\n      \n    \n    \n      \n        11.95\n      \n    \n    \n      \n        12.00\n      \n    \n    \n      \n        12.05\n      \n    \n    \n      \n        12.10\n      \n    \n    \n      \n        12.15\n      \n    \n    \n      \n        12.20\n      \n    \n    \n      \n        12.25\n      \n    \n    \n      \n        12.30\n      \n    \n    \n      \n        12.35\n      \n    \n    \n      \n        12.40\n      \n    \n    \n      \n        12.45\n      \n    \n    \n      \n        12.50\n      \n    \n    \n      \n        12.55\n      \n    \n    \n      \n        12.60\n      \n    \n    \n      \n        12.65\n      \n    \n    \n      \n        12.70\n      \n    \n    \n      \n        12.75\n      \n    \n    \n      \n        12.80\n      \n    \n    \n      \n        12.85\n      \n    \n    \n      \n        12.90\n      \n    \n    \n      \n        12.95\n      \n    \n    \n      \n        13.00\n      \n    \n    \n      \n        13.05\n      \n    \n    \n      \n        13.10\n      \n    \n    \n      \n        13.15\n      \n    \n    \n      \n        13.20\n      \n    \n    \n      \n        13.25\n      \n    \n    \n      \n        13.30\n      \n    \n    \n      \n        13.35\n      \n    \n    \n      \n        13.40\n      \n    \n    \n      \n        13.45\n      \n    \n    \n      \n        13.50\n      \n    \n    \n      \n        13.55\n      \n    \n    \n      \n        13.60\n      \n    \n    \n      \n        13.65\n      \n    \n    \n      \n        13.70\n      \n    \n    \n      \n        13.75\n      \n    \n    \n      \n        13.80\n      \n    \n    \n      \n        13.85\n      \n    \n    \n      \n        13.90\n      \n    \n    \n      \n        13.95\n      \n    \n    \n      \n        14.00\n      \n    \n    \n      \n        14.05\n      \n    \n    \n      \n        14.10\n      \n    \n    \n      \n        14.15\n      \n    \n    \n      \n        14.20\n      \n    \n    \n      \n        14.25\n      \n    \n    \n      \n        14.30\n      \n    \n    \n      \n        14.35\n      \n    \n    \n      \n        14.40\n      \n    \n    \n      \n        14.45\n      \n    \n    \n      \n        14.50\n      \n    \n    \n      \n        14.55\n      \n    \n    \n      \n        14.60\n      \n    \n    \n      \n        14.65\n      \n    \n    \n      \n        14.70\n      \n    \n    \n      \n        14.75\n      \n    \n    \n      \n        14.80\n      \n    \n    \n      \n        14.85\n      \n    \n    \n      \n        14.90\n      \n    \n    \n      \n        14.95\n      \n    \n    \n      \n        15.00\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        20\n      \n    \n  \n  \n    \n      \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n          \n        \n        \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  15,0.997\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  14,0.983\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  13,0.983\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  12,0.971\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  11,0.945\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  10,0.956\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  9,0.924\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  8,0.874\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  7,0.823\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  6,0.743\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5,0.647\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4,0.5\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3,0.356\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2,0.205\n                \n              \n            \n          \n        \n        \n          \n            \n              \n            \n          \n        \n        \n          \n            \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n          \n            \n              \n                h,j,k,l,arrows,drag to pan\n              \n            \n            \n              \n                i,o,+,-,scroll,shift-drag to zoom\n              \n            \n            \n              \n                r,dbl-click to reset\n              \n            \n            \n              \n                c for coordinates\n              \n            \n            \n              \n                ? for help\n              \n            \n          \n        \n      \n      \n        \n          \n            \n              ?\n            \n          \n        \n      \n    \n  \n  \n    \n      \n        0.0\n      \n    \n    \n      \n        0.5\n      \n    \n    \n      \n        1.0\n      \n    \n    \n      \n        0.00\n      \n    \n    \n      \n        0.05\n      \n    \n    \n      \n        0.10\n      \n    \n    \n      \n        0.15\n      \n    \n    \n      \n        0.20\n      \n    \n    \n      \n        0.25\n      \n    \n    \n      \n        0.30\n      \n    \n    \n      \n        0.35\n      \n    \n    \n      \n        0.40\n      \n    \n    \n      \n        0.45\n      \n    \n    \n      \n        0.50\n      \n    \n    \n      \n        0.55\n      \n    \n    \n      \n        0.60\n      \n    \n    \n      \n        0.65\n      \n    \n    \n      \n        0.70\n      \n    \n    \n      \n        0.75\n      \n    \n    \n      \n        0.80\n      \n    \n    \n      \n        0.85\n      \n    \n    \n      \n        0.90\n      \n    \n    \n      \n        0.95\n      \n    \n    \n      \n        1.00\n      \n    \n    \n      \n        0.000\n      \n    \n    \n      \n        0.005\n      \n    \n    \n      \n        0.010\n      \n    \n    \n      \n        0.015\n      \n    \n    \n      \n        0.020\n      \n    \n    \n      \n        0.025\n      \n    \n    \n      \n        0.030\n      \n    \n    \n      \n        0.035\n      \n    \n    \n      \n        0.040\n      \n    \n    \n      \n        0.045\n      \n    \n    \n      \n        0.050\n      \n    \n    \n      \n        0.055\n      \n    \n    \n      \n        0.060\n      \n    \n    \n      \n        0.065\n      \n    \n    \n      \n        0.070\n      \n    \n    \n      \n        0.075\n      \n    \n    \n      \n        0.080\n      \n    \n    \n      \n        0.085\n      \n    \n    \n      \n        0.090\n      \n    \n    \n      \n        0.095\n      \n    \n    \n      \n        0.100\n      \n    \n    \n      \n        0.105\n      \n    \n    \n      \n        0.110\n      \n    \n    \n      \n        0.115\n      \n    \n    \n      \n        0.120\n      \n    \n    \n      \n        0.125\n      \n    \n    \n      \n        0.130\n      \n    \n    \n      \n        0.135\n      \n    \n    \n      \n        0.140\n      \n    \n    \n      \n        0.145\n      \n    \n    \n      \n        0.150\n      \n    \n    \n      \n        0.155\n      \n    \n    \n      \n        0.160\n      \n    \n    \n      \n        0.165\n      \n    \n    \n      \n        0.170\n      \n    \n    \n      \n        0.175\n      \n    \n    \n      \n        0.180\n      \n    \n    \n      \n        0.185\n      \n    \n    \n      \n        0.190\n      \n    \n    \n      \n        0.195\n      \n    \n    \n      \n        0.200\n      \n    \n    \n      \n        0.205\n      \n    \n    \n      \n        0.210\n      \n    \n    \n      \n        0.215\n      \n    \n    \n      \n        0.220\n      \n    \n    \n      \n        0.225\n      \n    \n    \n      \n        0.230\n      \n    \n    \n      \n        0.235\n      \n    \n    \n      \n        0.240\n      \n    \n    \n      \n        0.245\n      \n    \n    \n      \n        0.250\n      \n    \n    \n      \n        0.255\n      \n    \n    \n      \n        0.260\n      \n    \n    \n      \n        0.265\n      \n    \n    \n      \n        0.270\n      \n    \n    \n      \n        0.275\n      \n    \n    \n      \n        0.280\n      \n    \n    \n      \n        0.285\n      \n    \n    \n      \n        0.290\n      \n    \n    \n      \n        0.295\n      \n    \n    \n      \n        0.300\n      \n    \n    \n      \n        0.305\n      \n    \n    \n      \n        0.310\n      \n    \n    \n      \n        0.315\n      \n    \n    \n      \n        0.320\n      \n    \n    \n      \n        0.325\n      \n    \n    \n      \n        0.330\n      \n    \n    \n      \n        0.335\n      \n    \n    \n      \n        0.340\n      \n    \n    \n      \n        0.345\n      \n    \n    \n      \n        0.350\n      \n    \n    \n      \n        0.355\n      \n    \n    \n      \n        0.360\n      \n    \n    \n      \n        0.365\n      \n    \n    \n      \n        0.370\n      \n    \n    \n      \n        0.375\n      \n    \n    \n      \n        0.380\n      \n    \n    \n      \n        0.385\n      \n    \n    \n      \n        0.390\n      \n    \n    \n      \n        0.395\n      \n    \n    \n      \n        0.400\n      \n    \n    \n      \n        0.405\n      \n    \n    \n      \n        0.410\n      \n    \n    \n      \n        0.415\n      \n    \n    \n      \n        0.420\n      \n    \n    \n      \n        0.425\n      \n    \n    \n      \n        0.430\n      \n    \n    \n      \n        0.435\n      \n    \n    \n      \n        0.440\n      \n    \n    \n      \n        0.445\n      \n    \n    \n      \n        0.450\n      \n    \n    \n      \n        0.455\n      \n    \n    \n      \n        0.460\n      \n    \n    \n      \n        0.465\n      \n    \n    \n      \n        0.470\n      \n    \n    \n      \n        0.475\n      \n    \n    \n      \n        0.480\n      \n    \n    \n      \n        0.485\n      \n    \n    \n      \n        0.490\n      \n    \n    \n      \n        0.495\n      \n    \n    \n      \n        0.500\n      \n    \n    \n      \n        0.505\n      \n    \n    \n      \n        0.510\n      \n    \n    \n      \n        0.515\n      \n    \n    \n      \n        0.520\n      \n    \n    \n      \n        0.525\n      \n    \n    \n      \n        0.530\n      \n    \n    \n      \n        0.535\n      \n    \n    \n      \n        0.540\n      \n    \n    \n      \n        0.545\n      \n    \n    \n      \n        0.550\n      \n    \n    \n      \n        0.555\n      \n    \n    \n      \n        0.560\n      \n    \n    \n      \n        0.565\n      \n    \n    \n      \n        0.570\n      \n    \n    \n      \n        0.575\n      \n    \n    \n      \n        0.580\n      \n    \n    \n      \n        0.585\n      \n    \n    \n      \n        0.590\n      \n    \n    \n      \n        0.595\n      \n    \n    \n      \n        0.600\n      \n    \n    \n      \n        0.605\n      \n    \n    \n      \n        0.610\n      \n    \n    \n      \n        0.615\n      \n    \n    \n      \n        0.620\n      \n    \n    \n      \n        0.625\n      \n    \n    \n      \n        0.630\n      \n    \n    \n      \n        0.635\n      \n    \n    \n      \n        0.640\n      \n    \n    \n      \n        0.645\n      \n    \n    \n      \n        0.650\n      \n    \n    \n      \n        0.655\n      \n    \n    \n      \n        0.660\n      \n    \n    \n      \n        0.665\n      \n    \n    \n      \n        0.670\n      \n    \n    \n      \n        0.675\n      \n    \n    \n      \n        0.680\n      \n    \n    \n      \n        0.685\n      \n    \n    \n      \n        0.690\n      \n    \n    \n      \n        0.695\n      \n    \n    \n      \n        0.700\n      \n    \n    \n      \n        0.705\n      \n    \n    \n      \n        0.710\n      \n    \n    \n      \n        0.715\n      \n    \n    \n      \n        0.720\n      \n    \n    \n      \n        0.725\n      \n    \n    \n      \n        0.730\n      \n    \n    \n      \n        0.735\n      \n    \n    \n      \n        0.740\n      \n    \n    \n      \n        0.745\n      \n    \n    \n      \n        0.750\n      \n    \n    \n      \n        0.755\n      \n    \n    \n      \n        0.760\n      \n    \n    \n      \n        0.765\n      \n    \n    \n      \n        0.770\n      \n    \n    \n      \n        0.775\n      \n    \n    \n      \n        0.780\n      \n    \n    \n      \n        0.785\n      \n    \n    \n      \n        0.790\n      \n    \n    \n      \n        0.795\n      \n    \n    \n      \n        0.800\n      \n    \n    \n      \n        0.805\n      \n    \n    \n      \n        0.810\n      \n    \n    \n      \n        0.815\n      \n    \n    \n      \n        0.820\n      \n    \n    \n      \n        0.825\n      \n    \n    \n      \n        0.830\n      \n    \n    \n      \n        0.835\n      \n    \n    \n      \n        0.840\n      \n    \n    \n      \n        0.845\n      \n    \n    \n      \n        0.850\n      \n    \n    \n      \n        0.855\n      \n    \n    \n      \n        0.860\n      \n    \n    \n      \n        0.865\n      \n    \n    \n      \n        0.870\n      \n    \n    \n      \n        0.875\n      \n    \n    \n      \n        0.880\n      \n    \n    \n      \n        0.885\n      \n    \n    \n      \n        0.890\n      \n    \n    \n      \n        0.895\n      \n    \n    \n      \n        0.900\n      \n    \n    \n      \n        0.905\n      \n    \n    \n      \n        0.910\n      \n    \n    \n      \n        0.915\n      \n    \n    \n      \n        0.920\n      \n    \n    \n      \n        0.925\n      \n    \n    \n      \n        0.930\n      \n    \n    \n      \n        0.935\n      \n    \n    \n      \n        0.940\n      \n    \n    \n      \n        0.945\n      \n    \n    \n      \n        0.950\n      \n    \n    \n      \n        0.955\n      \n    \n    \n      \n        0.960\n      \n    \n    \n      \n        0.965\n      \n    \n    \n      \n        0.970\n      \n    \n    \n      \n        0.975\n      \n    \n    \n      \n        0.980\n      \n    \n    \n      \n        0.985\n      \n    \n    \n      \n        0.990\n      \n    \n    \n      \n        0.995\n      \n    \n    \n      \n        1.000\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        1\n      \n    \n  \n  \n    \n      \n        power\n      \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\nOur power analysis has determined that, with the parameters established above, we need ~8 or more participants to reliably detect an effect!\nThe code used above is specific to power analysis for mixed models, but the approach generalises to other methods too, of course! The above code can easily be wrangled to handle different model types (simply change the model definition in single_run and make sure to capture the right parameters), and even Bayesian approaches. (For a thorough example of doing power analysis with Bayesian methods and the awesome bayesian regression package brms, see Kurz (2021).)\nEven if the above code is spaghetti to you (Perhaps you prefer R? Or Python?), I hope you will take away a few things from this tutorial:\n\nPower analysis is nothing more than testing whether we can recover the parameters of a hypothesised data-generating process reliably using our statistical test of choice.\nWe can determine the parameters for such a data-generating process in the same way we formulate hypotheses (and indeed, in some ways these two things are one and the same): we use our knowledge, intuition, and previous work to inform our decision-making.\nIf you have a hypothetical data-generating process, you can simulate data by simply formalising that process as code and letting it simulate a dataset\nSimulation can help you answer questions about your statistical approach that are difficult to answer with other tools\n\n\nReferences\n\n\nAdam, K. C. S., Patel, T., Rangan, N., & Serences, J. T. (2021). Classic Visual Search Effects in an Additional Singleton Task: An Open Dataset. 4(1), 34. https://doi.org/10.5334/joc.182\n\n\nDeBruine, L. M., & Barr, D. J. (2020). Appendix 1c: Sensitivity Analysis. https://debruine.github.io/lmem_sim/articles/appendix1c_sensitivity.html.\n\n\nDeBruine, L. M., & Barr, D. J. (2021). Understanding Mixed-Effects Models Through Data Simulation. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920965119. https://doi.org/10.1177/2515245920965119\n\n\nKurz, A. S. (2021). Bayesian power analysis: Part I. Prepare to reject ‘\\(H_0\\)‘ with simulation. In A. Solomon Kurz. https://solomonkurz.netlify.app/blog/bayesian-power-analysis-part-i/.\n\n\nLakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence Testing for Psychological Research: A Tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963"
  }
]